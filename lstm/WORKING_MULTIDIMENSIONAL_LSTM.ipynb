{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef77dc2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T06:01:09.323160Z",
     "start_time": "2021-11-23T06:01:08.826469Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3a7b285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T06:21:52.344490Z",
     "start_time": "2021-11-23T06:21:52.325113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [1]]\n",
      "--\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "dataset=StringIO(\"\"\"Date,Open,High,Low,Close,Volume,Trade_count,Vwap\n",
    "2015-12-01 09:00:00+00:00,118.88,118.94,118.88,118.94,1145,5,118.902052\n",
    "2015-12-01 09:15:00+00:00,118.77,118.77,118.77,118.77,200,1,118.77\n",
    "2015-12-01 09:30:00+00:00,118.69,118.69,118.6,118.6,900,4,118.61\n",
    "2015-12-01 09:45:00+00:00,118.64,118.65,118.64,118.65,3580,5,118.648883\n",
    "2015-12-01 10:00:00+00:00,118.65,118.65,118.55,118.55,1820,4,118.611538\n",
    "2015-12-01 10:15:00+00:00,118.55,118.6,118.55,118.6,880,5,118.5625\n",
    "2015-12-01 10:30:00+00:00,118.55,118.55,118.5,118.5,1878,5,118.513312\n",
    "2015-12-01 10:45:00+00:00,118.59,118.72,118.59,118.72,2499,10,118.628431\n",
    "2015-12-01 11:00:00+00:00,118.71,118.9,118.71,118.9,2842,11,118.86064\n",
    "2015-12-01 11:15:00+00:00,118.87,118.87,118.87,118.87,300,2,118.87\n",
    "2015-12-01 11:30:00+00:00,118.78,118.8,118.76,118.8,3914,22,118.785876\n",
    "2015-12-01 11:45:00+00:00,118.8,118.99,118.77,118.9,7900,37,118.893542\n",
    "2015-12-01 12:00:00+00:00,118.88,118.98,118.84,118.84,6540,34,118.922648\n",
    "2015-12-01 12:15:00+00:00,118.82,118.84,118.77,118.77,5603,28,118.804962\n",
    "2015-12-01 12:30:00+00:00,118.77,118.89,118.76,118.88,7612,31,118.824002\n",
    "\"\"\")\n",
    "df = pd.read_table(dataset, sep=\",\")\n",
    "\n",
    "#ip = np.array([ [1,2,3],[6,8,9],[3,4,5],[4,7,8],[4,2,5],[5,7,4] ])\n",
    "#op = np.array([[2,8,4,7,2,4]])\n",
    "#op = op.reshape(6,1)\n",
    "ip = np.array([ [1],  [2],  [0],  [2],  [0],  [1],  [2],  [1] ])\n",
    "op = np.array([ [300],[100],[200],[100],[200],[300],[100],[300] ])\n",
    "num_steps = 3\n",
    "num_features = 3\n",
    "#ip_shaped = np.reshape(ip, newshape=(-1, num_steps, num_features))\n",
    "\n",
    "#X = np.array([ [1,2,3, 4, 5, 6] ])\n",
    "#Y = np.array([[2,3,4,5,6,7]])\n",
    "print(ip)\n",
    "print(\"--\")\n",
    "#ip= np.tile(ip,(50,1))\n",
    "#op = np.tile(op,(50,1))\n",
    "print(len(op))\n",
    "#ip = np.array([ [1],[1],[1],[1],[1],[2],[1],[2]])\n",
    "#op = np.array([ [1,    0,    1,    0,    1,    1,    0,    1 ]]).T\n",
    "\n",
    "ip = np.array([ [1,1,2],[1,2,1],[1,0,2],[1,2,1],[1,0,2],[1,1,2],[1,2,1],[1,1,2]])\n",
    "op = np.array([ [1,    0,    1,    0,    1,    1,    0,    1 ]]).T\n",
    "#ip = np.array([ [1,1,1,1,1],[0,2,2,2,2],[1,3,3,3,3],[1,4,4,4,4],[0,5,5,5,5],[0,6,6,6,6],[1,7,7,7,7],[0,8,8,8,8]])\n",
    "#op = np.array([ [1,          0,          1,          1,          0,          0,          1,          0 ]]).T\n",
    "_, numFeats = ip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1581704c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T07:34:13.166751Z",
     "start_time": "2021-11-23T07:34:13.152527Z"
    }
   },
   "outputs": [],
   "source": [
    "def lstm_data_transform(x_data, y_data, timeSteps=2):\n",
    "    \"\"\" Changes data to the format for LSTM training \n",
    "for sliding window approach \"\"\"\n",
    "    # Prepare the list for the transformed data\n",
    "    X, y = list(), list()\n",
    "    # Loop of the entire data set\n",
    "    #print(x_data.shape[0])\n",
    "    for i in range(x_data.shape[0]):\n",
    "        # compute a new (sliding window) index\n",
    "        end_ix = i + timeSteps\n",
    "\n",
    "        # if index is larger than the size of the dataset, we stop\n",
    "        #print(end_ix)\n",
    "        if end_ix >= x_data.shape[0]:\n",
    "            break\n",
    "        # Get a sequence of data for x\n",
    "        seq_X = x_data[i:end_ix]\n",
    "        #print(x_data[i:end_ix])\n",
    "        # Get only the last element of the sequency for y\n",
    "        #print(y_data[end_ix])\n",
    "        seq_y = y_data[i:end_ix]\n",
    "        # Append the list with sequencies\n",
    "        X.append(seq_X)\n",
    "        y.append(seq_y)\n",
    "        display(X)\n",
    "        print(\"--\")\n",
    "    # Make final arrays\n",
    "    x_array = np.array(X)\n",
    "    y_array = np.array(y)\n",
    "    return x_array, y_array\n",
    "\n",
    "def lstm_data_time(x_data, y_data, timeSteps=2):\n",
    "    \"\"\" Changes data to the format for LSTM training \n",
    "for sliding window approach \"\"\"\n",
    "    # Prepare the list for the transformed data\n",
    "    X, y = list(), list()\n",
    "    # Loop of the entire data set\n",
    "    #print(x_data.shape[0])\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "\n",
    "    for i in range(x_data.shape[0]):\n",
    "        my2, mx2 = list(), list()\n",
    "        # compute a new (sliding window) index\n",
    "        end_ix = i + timeSteps\n",
    "\n",
    "        # if index is larger than the size of the dataset, we stop\n",
    "        #print(end_ix)\n",
    "        if end_ix >= x_data.shape[0]:\n",
    "            break\n",
    "\n",
    "        for j in range(i, i+timeSteps):\n",
    "            my1, mx1 = list(), list()\n",
    "            # Get a sequence of data for x\n",
    "            seq_X = x_data[j]\n",
    "            # Get only the last element of the sequency for y\n",
    "            seq_y = y_data[j]\n",
    "            # Append the list with sequencies\n",
    "            for k in range(len(seq_X)):\n",
    "                my, mx = list(), list()\n",
    "                mx.append(seq_X[k])\n",
    "                mx1.append(mx)\n",
    "                \n",
    "            mx2.append(mx1)\n",
    "            my2.append(seq_y)\n",
    "            \n",
    "        X.append(mx2)\n",
    "        y.append(my2)\n",
    "\n",
    "        #X.append(np.array(mx1))\n",
    "        #y.append(np.array(my))\n",
    "        print(\"--\")\n",
    "    # Make final arrays\n",
    "    x_array = np.array(X)\n",
    "    y_array = np.array(y)\n",
    "\n",
    "    return x_array, y_array\n",
    "#3 3 1 want\n",
    "#3 3 3 now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8de04139",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T07:45:29.252456Z",
     "start_time": "2021-11-23T07:45:29.240214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4)\n",
      "--\n",
      "--\n",
      "--\n",
      "--\n",
      "--\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [2]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [2],\n",
       "         [1]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [2]]],\n",
       "\n",
       "\n",
       "       [[[1],\n",
       "         [1],\n",
       "         [2],\n",
       "         [1]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [2]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [2],\n",
       "         [1]]],\n",
       "\n",
       "\n",
       "       [[[1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [2]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [2],\n",
       "         [1]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [2]]],\n",
       "\n",
       "\n",
       "       [[[1],\n",
       "         [1],\n",
       "         [2],\n",
       "         [1]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [2]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [2]]],\n",
       "\n",
       "\n",
       "       [[[1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [2]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [2]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [2],\n",
       "         [1]]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3, 4, 1)\n",
      "(3, 4, 1)\n",
      "[[[1]\n",
      "  [1]\n",
      "  [1]\n",
      "  [2]]\n",
      "\n",
      " [[1]\n",
      "  [1]\n",
      "  [2]\n",
      "  [1]]\n",
      "\n",
      " [[1]\n",
      "  [1]\n",
      "  [0]\n",
      "  [2]]]\n",
      "(5, 3, 1)\n",
      "====\n",
      "[[1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "timeSteps = 3\n",
    "ip = np.array([ [1,1,1,2],[1,1,2,1],[1,1,0,2],[1,1,2,1],[1,1,0,2],[1,1,1,2],[1,1,2,1],[1,1,1,2]])\n",
    "print(ip.shape)\n",
    "ipt,opt=lstm_data_time(ip,op, timeSteps)\n",
    "display(ipt)\n",
    "print(ipt.shape)\n",
    "print(ipt[0].shape)\n",
    "print(ipt[0])\n",
    "print(opt.shape)\n",
    "\n",
    "print(\"====\")\n",
    "print(opt[0])\n",
    "# batchSize = len(ipt)\n",
    "# [[[1.]\n",
    "#   [0.]\n",
    "#   [0.]]\n",
    "\n",
    "#  [[1.]\n",
    "#   [0.]\n",
    "#   [0.]]\n",
    "\n",
    "#  [[1.]\n",
    "#   [0.]\n",
    "#   [0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9912c5b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T06:01:39.169544Z",
     "start_time": "2021-11-23T06:01:39.151629Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the element-wise sigmoid activation function for an array x.\n",
    "\n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    f = 1 / (1 + np.exp(-x_safe))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        return f * (1 - f)\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f\n",
    "\n",
    "\n",
    "\n",
    "def init_orthogonal(param):\n",
    "    \"\"\"\n",
    "    Initializes weight parameters orthogonally.\n",
    "    This is a common initiailization for recurrent neural networks.\n",
    "    \n",
    "    Refer to this paper for an explanation of this initialization:\n",
    "    https://arxiv.org/abs/1312.6120\n",
    "    \"\"\"\n",
    "    if param.ndim < 2:\n",
    "        raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
    "\n",
    "    rows, cols = param.shape\n",
    "    \n",
    "    new_param = np.random.randn(rows, cols)\n",
    "    \n",
    "    if rows < cols:\n",
    "        new_param = new_param.T\n",
    "    \n",
    "    # Compute QR factorization\n",
    "    q, r = np.linalg.qr(new_param)\n",
    "    \n",
    "    # Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\n",
    "    d = np.diag(r, 0)\n",
    "    ph = np.sign(d)\n",
    "    q *= ph\n",
    "\n",
    "    if rows < cols:\n",
    "        q = q.T\n",
    "    \n",
    "    new_param = q\n",
    "    \n",
    "    return new_param\n",
    "\n",
    "  \n",
    "def tanh(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the element-wise tanh activation function for an array x.\n",
    "\n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    f = (np.exp(x_safe)-np.exp(-x_safe))/(np.exp(x_safe)+np.exp(-x_safe))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        return 1-f**2\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f\n",
    "    \n",
    "    \n",
    "def softmax(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the softmax for an array x.\n",
    "    \n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    f = np.exp(x_safe) / np.sum(np.exp(x_safe))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        pass # We will not need this one\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f\n",
    "    \n",
    "def clip_gradient_norm(grads, max_norm=0.25):\n",
    "    \"\"\"\n",
    "    Clips gradients to have a maximum norm of `max_norm`.\n",
    "    This is to prevent the exploding gradients problem.\n",
    "    \"\"\" \n",
    "    # Set the maximum of the norm to be of type float\n",
    "    max_norm = float(max_norm)\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculate the L2 norm squared for each gradient and add them to the total norm\n",
    "    for grad in grads:\n",
    "        grad_norm = np.sum(np.power(grad, 2))\n",
    "        total_norm += grad_norm\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Calculate clipping coeficient\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    \n",
    "    # If the total norm is larger than the maximum allowable norm, then clip the gradient\n",
    "    if clip_coef < 1:\n",
    "        for grad in grads:\n",
    "            grad *= clip_coef\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def update_parameters(params, grads, lr=1e-3):\n",
    "    # Take a step\n",
    "    for param, grad in zip(params, grads):\n",
    "        param -= lr * grad\n",
    "    \n",
    "    return params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c8c5e9d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T07:45:48.904987Z",
     "start_time": "2021-11-23T07:45:48.882137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wxi: (1, 3)\n",
      "wxf: (1, 3)\n",
      "wxc: (1, 3)\n",
      "wxo: (1, 3)\n",
      "whi: (6, 3)\n",
      "whf: (6, 3)\n",
      "whc: (6, 3)\n",
      "who: (6, 3)\n",
      "wy: (6, 1)\n",
      "bi: (3, 1)\n",
      "bf: (6, 1)\n",
      "bc: (6, 1)\n",
      "bo: (6, 1)\n",
      "by: (6, 1)\n"
     ]
    }
   ],
   "source": [
    "od=1\n",
    "def init_lstm(hd, timeSteps):\n",
    "    \"\"\"\n",
    "    Initializes our LSTM network.\n",
    "    \n",
    "    Args:\n",
    "     `hidden_size`: the dimensions of the hidden state\n",
    "     `vocab_size`: the dimensions of our vocabulary\n",
    "     `z_size`: the dimensions of the concatenated input \n",
    "    \"\"\"\n",
    "    z=hd+timeSteps\n",
    "    \n",
    "    wxi = 2*np.random.random((1,numFeats)) - 1\n",
    "    wxf = 2*np.random.random((1,numFeats)) - 1\n",
    "    wxc = 2*np.random.random((1,numFeats)) - 1\n",
    "    wxo = 2*np.random.random((1,numFeats)) - 1\n",
    "\n",
    "    whi= 2*np.random.random((hd,numFeats)) - 1\n",
    "    whf= 2*np.random.random((hd,numFeats)) - 1\n",
    "    whc= 2*np.random.random((hd,numFeats)) - 1\n",
    "    who = 2*np.random.random((hd,numFeats)) - 1\n",
    "\n",
    "    wy = 2*np.random.random((hd,1)) - 1\n",
    "\n",
    "    \n",
    "    bi = np.zeros((numFeats, 1))\n",
    "    bf = np.zeros((hd, 1))\n",
    "    bc = np.zeros((hd, 1))\n",
    "    bo = np.zeros((hd, 1))\n",
    "    \n",
    "    by = np.zeros((hd, 1))\n",
    "    \n",
    "\n",
    "    return wxi, wxf, wxc, wxo, whi, whf, whc, who, wy, bi, bf, bc, bo, by\n",
    "\n",
    "\n",
    "params = init_lstm(6, timeSteps)\n",
    "print('wxi:', params[0].shape)\n",
    "print('wxf:', params[1].shape)\n",
    "print('wxc:', params[2].shape)\n",
    "print('wxo:', params[3].shape)\n",
    "\n",
    "print('whi:', params[4].shape)\n",
    "print('whf:', params[5].shape)\n",
    "print('whc:', params[6].shape)\n",
    "print('who:', params[7].shape)\n",
    "\n",
    "print('wy:', params[8].shape)\n",
    "\n",
    "print('bi:', params[9].shape)\n",
    "print('bf:', params[10].shape)\n",
    "print('bc:', params[11].shape)\n",
    "print('bo:', params[12].shape)\n",
    "print('by:', params[13].shape)\n",
    "\n",
    "for param in params:\n",
    "    assert param.ndim == 2, \\\n",
    "        'all parameters should be 2-dimensional '\\\n",
    "        '(hint: a dimension can simply have size 1)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfcd3cb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T06:01:42.132582Z",
     "start_time": "2021-11-23T06:01:42.120953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A single sample from the generated dataset:\n",
      "['1', '1', '1', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set seed such that we always get the same dataset\n",
    "# (this is a good idea in general)\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_dataset(num_sequences=2**8):\n",
    "    \"\"\"\n",
    "    Generates a number of sequences as our dataset.\n",
    "    \n",
    "    Args:\n",
    "     `num_sequences`: the number of sequences to be generated.\n",
    "     \n",
    "    Returns a list of sequences.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    for _ in range(num_sequences): \n",
    "        num_tokens = np.random.randint(1, 12)\n",
    "        sample = ['1'] * num_tokens + ['0'] * num_tokens\n",
    "        samples.append(sample)\n",
    "        \n",
    "    return samples\n",
    "\n",
    "\n",
    "sequences = generate_dataset()\n",
    "\n",
    "print('A single sample from the generated dataset:')\n",
    "print(sequences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "7870a92d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T08:18:30.375311Z",
     "start_time": "2021-11-23T08:18:30.365922Z"
    }
   },
   "outputs": [],
   "source": [
    "# Size of concatenated hidden + input vector\n",
    "hd = 50\n",
    "numFeats = ind = 4\n",
    "z = hd + ind\n",
    "\n",
    "def init_lstm(hd, ind, z):\n",
    "    \"\"\"\n",
    "    Initializes our LSTM network.\n",
    "    \n",
    "    Args:\n",
    "     `hidden_size`: the dimensions of the hidden state\n",
    "     `vocab_size`: the dimensions of our vocabulary\n",
    "     `z_size`: the dimensions of the concatenated input \n",
    "    \"\"\"\n",
    "    # Weight matrix (forget gate)\n",
    "    W_f = np.zeros((hd, z))\n",
    "    # Weight matrix (input gate)\n",
    "    W_i = np.zeros((hd, z))\n",
    "    # Weight matrix (candidate)\n",
    "    W_g = np.zeros((hd, z))\n",
    "   \n",
    "    # Bias for forget gate\n",
    "    b_f = np.zeros((hd, 1))\n",
    "\n",
    "    \n",
    "    # Bias for input gate\n",
    "    b_i = np.zeros((hd, 1))\n",
    "\n",
    "\n",
    "    # Bias for candidate\n",
    "    b_g = np.zeros((hd, 1))\n",
    "\n",
    "    # Weight matrix of the output gate\n",
    "    W_o = np.zeros((hd, z))\n",
    "    \n",
    "    # Bias for output gate\n",
    "    b_o = np.zeros((hd, 1))\n",
    "\n",
    "    # Weight matrix relating the hidden-state to the output\n",
    "    W_v = np.zeros((1, hd))\n",
    "    \n",
    "    # Bias for logits\n",
    "    b_v = np.zeros((1, 1))\n",
    "    \n",
    "    # Initialize weights according to https://arxiv.org/abs/1312.6120\n",
    "    W_f = init_orthogonal(W_f)\n",
    "    W_i = init_orthogonal(W_i)\n",
    "    W_g = init_orthogonal(W_g)\n",
    "    W_o = init_orthogonal(W_o)\n",
    "    W_v = init_orthogonal(W_v)\n",
    "\n",
    "    return W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "63f18e3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T08:19:14.240306Z",
     "start_time": "2021-11-23T08:19:14.219649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "[[[1]\n",
      "  [1]\n",
      "  [1]\n",
      "  [2]]\n",
      "\n",
      " [[1]\n",
      "  [1]\n",
      "  [2]\n",
      "  [1]]\n",
      "\n",
      " [[1]\n",
      "  [1]\n",
      "  [0]\n",
      "  [2]]]\n",
      "Target:\n",
      "[[1]\n",
      " [0]\n",
      " [1]]\n",
      "Predicted:\n",
      "[[[1.0006564 ]]\n",
      "\n",
      " [[0.99887137]]\n",
      "\n",
      " [[0.99909264]]]\n"
     ]
    }
   ],
   "source": [
    "def forward(inputs, h_prev, C_prev, p):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    h_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    C_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    p -- python list containing:\n",
    "                        W_f -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_f -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                        W_i -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_i -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "                        W_g -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_g --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                        W_o -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_o --  Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "                        W_v -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_v, n_a)\n",
    "                        b_v -- Bias relating the hidden-state to the output, numpy array of shape (n_v, 1)\n",
    "    Returns:\n",
    "    z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s -- lists of size m containing the computations in each forward pass\n",
    "    outputs -- prediction at timestep \"t\", numpy array of shape (n_v, m)\n",
    "    \"\"\"\n",
    "    assert h_prev.shape == (hd, 1)\n",
    "    assert C_prev.shape == (hd, 1)\n",
    "\n",
    "    # First we unpack our parameters\n",
    "    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p\n",
    "    \n",
    "    # Save a list of computations for each of the components in the LSTM\n",
    "    x_s, z_s, f_s, i_s,  = [], [] ,[], []\n",
    "    g_s, C_s, o_s, h_s = [], [] ,[], []\n",
    "    v_s, output_s =  [], [] \n",
    "    \n",
    "    # Append the initial cell and hidden state to their respective lists\n",
    "    h_s.append(h_prev)\n",
    "    C_s.append(C_prev)\n",
    "    \n",
    "    for x in inputs:\n",
    "        # Concatenate input and hidden state\n",
    "        z = np.row_stack((h_prev, x))\n",
    "        z_s.append(z)\n",
    "        \n",
    "        # Calculate forget gate\n",
    "        f = sigmoid(np.dot(W_f, z) + b_f)\n",
    "        f_s.append(f)\n",
    "        \n",
    "        # Calculate input gate\n",
    "        i = sigmoid(np.dot(W_i, z) + b_i)\n",
    "        i_s.append(i)\n",
    "        \n",
    "        # Calculate candidate\n",
    "        g = tanh(np.dot(W_g, z) + b_g)\n",
    "        g_s.append(g)\n",
    "        \n",
    "        # Calculate memory state\n",
    "        C_prev = C_prev * f + g * i\n",
    "        C_s.append(C_prev)\n",
    "        \n",
    "        # Calculate output gate\n",
    "        o = sigmoid(np.dot(W_o, z) + b_o)\n",
    "        o_s.append(o)\n",
    "        \n",
    "        # Calculate hidden state\n",
    "        h_prev = o * tanh(C_prev)\n",
    "        h_s.append(h_prev)\n",
    "\n",
    "        # Calculate logits\n",
    "        v = np.dot(W_v, h_prev) + b_v\n",
    "        v_s.append(v)\n",
    "\n",
    "        # Calculate softmax\n",
    "        #output = softmax(v)\n",
    "        output_s.append(v)\n",
    "\n",
    "    return z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, output_s\n",
    "\n",
    "\n",
    "# Get first sentence in test set\n",
    "inputs, targets = test_set[1]\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "h = np.zeros((hd, 1))\n",
    "c = np.zeros((hd, 1))\n",
    "\n",
    "# Forward pass\n",
    "z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(ipt[0], h, c, params)\n",
    "\n",
    "#output_sentence = [idx_to_word[np.argmax(output)] for output in outputs]\n",
    "print('Input:')\n",
    "print(ipt[0])\n",
    "\n",
    "print('Target:')\n",
    "print(opt[0])\n",
    "\n",
    "print('Predicted:')\n",
    "print(np.array(v_s))\n",
    "# print([idx_to_word[np.argmax(output)] for output in outputs])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "98254cac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T08:13:40.943338Z",
     "start_time": "2021-11-23T08:13:40.922492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.99959403]]), array([[0.99930368]]), array([[1.0007546]])]\n",
      "[[1]\n",
      " [0]\n",
      " [1]]\n",
      "We get a loss of:\n",
      "0.9986085745539479\n"
     ]
    }
   ],
   "source": [
    "def backward(z, f, i, g, C, o, h, v, outputs, targets, p = params):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    z -- your concatenated input data  as a list of size m.\n",
    "    f -- your forget gate computations as a list of size m.\n",
    "    i -- your input gate computations as a list of size m.\n",
    "    g -- your candidate computations as a list of size m.\n",
    "    C -- your Cell states as a list of size m+1.\n",
    "    o -- your output gate computations as a list of size m.\n",
    "    h -- your Hidden state computations as a list of size m+1.\n",
    "    v -- your logit computations as a list of size m.\n",
    "    outputs -- your outputs as a list of size m.\n",
    "    targets -- your targets as a list of size m.\n",
    "    p -- python list containing:\n",
    "                        W_f -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_f -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                        W_i -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_i -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "                        W_g -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_g --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                        W_o -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_o --  Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "                        W_v -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_v, n_a)\n",
    "                        b_v -- Bias relating the hidden-state to the output, numpy array of shape (n_v, 1)\n",
    "    Returns:\n",
    "    loss -- crossentropy loss for all elements in output\n",
    "    grads -- lists of gradients of every element in p\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack parameters\n",
    "    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p\n",
    "\n",
    "    # Initialize gradients as zero\n",
    "    W_f_d = np.zeros_like(W_f)\n",
    "    b_f_d = np.zeros_like(b_f)\n",
    "\n",
    "    W_i_d = np.zeros_like(W_i)\n",
    "    b_i_d = np.zeros_like(b_i)\n",
    "\n",
    "    W_g_d = np.zeros_like(W_g)\n",
    "    b_g_d = np.zeros_like(b_g)\n",
    "\n",
    "    W_o_d = np.zeros_like(W_o)\n",
    "    b_o_d = np.zeros_like(b_o)\n",
    "\n",
    "    W_v_d = np.zeros_like(W_v)\n",
    "    b_v_d = np.zeros_like(b_v)\n",
    "    \n",
    "    # Set the next cell and hidden state equal to zero\n",
    "    dh_next = np.zeros_like(h[0])\n",
    "    dC_next = np.zeros_like(C[0])\n",
    "        \n",
    "    # Track loss\n",
    "    loss = 0\n",
    "    \n",
    "    for t in reversed(range(len(outputs))):\n",
    "        # Compute the cross entropy\n",
    "        loss += -np.mean(np.log(outputs[t]) * targets[t])\n",
    "\n",
    "        # Get the previous hidden cell state\n",
    "        C_prev= C[t-1]\n",
    "\n",
    "        # Compute the derivative of the relation of the hidden-state to the output gate\n",
    "        dv = np.copy(outputs[t])\n",
    "        dv[np.argmax(targets[t])] -= 1\n",
    "\n",
    "        # Update the gradient of the relation of the hidden-state to the output gate\n",
    "        W_v_d += np.dot(dv, h[t].T)\n",
    "        b_v_d += dv\n",
    "\n",
    "        # Compute the derivative of the hidden state and output gate\n",
    "        dh = np.dot(W_v.T, dv)\n",
    "        dh += dh_next\n",
    "        do = dh * tanh(C[t])\n",
    "        do = sigmoid(o[t], derivative=True)*do\n",
    "\n",
    "        # Update the gradients with respect to the output gate\n",
    "        W_o_d += np.dot(do, z[t].T)\n",
    "        b_o_d += do\n",
    "\n",
    "        # Compute the derivative of the cell state and candidate g\n",
    "        dC = np.copy(dC_next)\n",
    "        dC += dh * o[t] * tanh(tanh(C[t]), derivative=True)\n",
    "        dg = dC * i[t]\n",
    "        dg = tanh(g[t], derivative=True) * dg\n",
    "\n",
    "        # Update the gradients with respect to the candidate\n",
    "        W_g_d += np.dot(dg, z[t].T)\n",
    "        b_g_d += dg\n",
    "\n",
    "        # Compute the derivative of the input gate and update its gradients\n",
    "        di = dC * g[t]\n",
    "        di = sigmoid(i[t], True) * di\n",
    "        W_i_d += np.dot(di, z[t].T)\n",
    "        b_i_d += di\n",
    "\n",
    "        # Compute the derivative of the forget gate and update its gradients\n",
    "        df = dC * C_prev\n",
    "        df = sigmoid(f[t]) * df\n",
    "        W_f_d += np.dot(df, z[t].T)\n",
    "        b_f_d += df\n",
    "\n",
    "        # Compute the derivative of the input and update the gradients of the previous hidden and cell state\n",
    "        dz = (np.dot(W_f.T, df)\n",
    "             + np.dot(W_i.T, di)\n",
    "             + np.dot(W_g.T, dg)\n",
    "             + np.dot(W_o.T, do))\n",
    "        dh_prev = dz[:hd, :]\n",
    "        dC_prev = f[t] * dC\n",
    "        \n",
    "    grads= W_f_d, W_i_d, W_g_d, W_o_d, W_v_d, b_f_d, b_i_d, b_g_d, b_o_d, b_v_d\n",
    "    \n",
    "    # Clip gradients\n",
    "    grads = clip_gradient_norm(grads)\n",
    "    \n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "# Perform a backward pass\n",
    "loss, grads = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, opt[0], params)\n",
    "print(outputs)\n",
    "print(opt[0])\n",
    "print('We get a loss of:')\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "6d58a6b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T08:20:04.072733Z",
     "start_time": "2021-11-23T08:19:37.956314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 0.04214851505304882, validation loss: 0.3986063178449619\n",
      "Epoch 10, training loss: 0.029931518905172066, validation loss: 0.24407418824899693\n",
      "Epoch 20, training loss: 0.029821800214628186, validation loss: 0.24397066952403917\n",
      "Epoch 30, training loss: 0.02980736031984061, validation loss: 0.2442878639106558\n",
      "Epoch 40, training loss: 0.02972276003941205, validation loss: 0.24370422959477672\n",
      "Epoch 50, training loss: 0.029615400921454472, validation loss: 0.24278632933476665\n",
      "Epoch 60, training loss: 0.02951804351057778, validation loss: 0.24190348269729608\n",
      "Epoch 70, training loss: 0.029441010994751642, validation loss: 0.24118274587381033\n",
      "Epoch 80, training loss: 0.029384124719823743, validation loss: 0.2406370808210111\n",
      "Epoch 90, training loss: 0.0293437960209227, validation loss: 0.2402400247216471\n",
      "Epoch 100, training loss: 0.02931600841867413, validation loss: 0.23995753354949934\n",
      "Epoch 110, training loss: 0.029297322914925657, validation loss: 0.23975923249848796\n",
      "Epoch 120, training loss: 0.02928508140652684, validation loss: 0.23962119021905243\n",
      "Epoch 130, training loss: 0.029277329880911657, validation loss: 0.2395256198243952\n",
      "Epoch 140, training loss: 0.02927267092165223, validation loss: 0.2394597066394229\n",
      "Epoch 150, training loss: 0.029270121146558536, validation loss: 0.23941438063875917\n",
      "Epoch 160, training loss: 0.029268995351404675, validation loss: 0.23938328883813248\n",
      "Epoch 170, training loss: 0.029268819140986363, validation loss: 0.2393620088586259\n",
      "Epoch 180, training loss: 0.02926926553594722, validation loss: 0.23934747344997184\n",
      "Epoch 190, training loss: 0.02927010996125135, validation loss: 0.23933755968412684\n",
      "Epoch 200, training loss: 0.0292711986892317, validation loss: 0.23933079988401387\n",
      "Epoch 210, training loss: 0.029272426882425734, validation loss: 0.23932617993933916\n",
      "Epoch 220, training loss: 0.029273723385001744, validation loss: 0.2393229993016184\n",
      "Epoch 230, training loss: 0.0292750402167741, validation loss: 0.239320774080376\n",
      "Epoch 240, training loss: 0.02927634532691491, validation loss: 0.23931917007942807\n",
      "Epoch 250, training loss: 0.029277617600214845, validation loss: 0.23931795655821542\n",
      "Epoch 260, training loss: 0.029278843417203118, validation loss: 0.23931697431099241\n",
      "Epoch 270, training loss: 0.029280014285178974, validation loss: 0.23931611362737343\n",
      "Epoch 280, training loss: 0.029281125207023154, validation loss: 0.2393152990696304\n",
      "Epoch 290, training loss: 0.02928217355825129, validation loss: 0.23931447895251715\n",
      "Epoch 300, training loss: 0.029283158314226457, validation loss: 0.2393136180679128\n",
      "Epoch 310, training loss: 0.029284079518670146, validation loss: 0.23931269264938382\n",
      "Epoch 320, training loss: 0.02928493791849561, validation loss: 0.23931168688381033\n",
      "Epoch 330, training loss: 0.029285734713310654, validation loss: 0.23931059049222758\n",
      "Epoch 340, training loss: 0.029286471383991007, validation loss: 0.23930939705016135\n",
      "Epoch 350, training loss: 0.029287149575780207, validation loss: 0.23930810281983397\n",
      "Epoch 360, training loss: 0.02928777101898614, validation loss: 0.23930670593699938\n",
      "Epoch 370, training loss: 0.029288337475591925, validation loss: 0.23930520584372103\n",
      "Epoch 380, training loss: 0.02928885070371684, validation loss: 0.2393036028919101\n",
      "Epoch 390, training loss: 0.029289312434358773, validation loss: 0.23930189806558264\n",
      "Epoch 400, training loss: 0.02928972435657186, validation loss: 0.23930009278577655\n",
      "Epoch 410, training loss: 0.029290088108422863, validation loss: 0.2392981887731275\n",
      "Epoch 420, training loss: 0.02929040527189101, validation loss: 0.23929618795074867\n",
      "Epoch 430, training loss: 0.029290677370444437, validation loss: 0.23929409237535007\n",
      "Epoch 440, training loss: 0.02929090586841854, validation loss: 0.23929190418820911\n",
      "Epoch 450, training loss: 0.02929109217159373, validation loss: 0.23928962558014052\n",
      "Epoch 460, training loss: 0.029291237628557305, validation loss: 0.23928725876638474\n",
      "Epoch 470, training loss: 0.029291343532565035, validation loss: 0.2392848059685584\n",
      "Epoch 480, training loss: 0.029291411123707135, validation loss: 0.2392822694016635\n",
      "Epoch 490, training loss: 0.02929144159124637, validation loss: 0.23927965126474843\n",
      "Epoch 500, training loss: 0.029291436076038596, validation loss: 0.23927695373422558\n",
      "Epoch 510, training loss: 0.029291395672975985, validation loss: 0.239274178959143\n",
      "Epoch 520, training loss: 0.029291321433413623, validation loss: 0.23927132905790785\n",
      "Epoch 530, training loss: 0.02929121436755455, validation loss: 0.23926840611610425\n",
      "Epoch 540, training loss: 0.029291075446777604, validation loss: 0.2392654121851465\n",
      "Epoch 550, training loss: 0.02929090560589958, validation loss: 0.2392623492815805\n",
      "Epoch 560, training loss: 0.029290705745367213, validation loss: 0.23925921938689604\n",
      "Epoch 570, training loss: 0.029290476733377833, validation loss: 0.2392560244477505\n",
      "Epoch 580, training loss: 0.02929021940792954, validation loss: 0.23925276637652573\n",
      "Epoch 590, training loss: 0.029289934578802856, validation loss: 0.23924944705216436\n",
      "Epoch 600, training loss: 0.029289623029476696, validation loss: 0.23924606832124137\n",
      "Epoch 610, training loss: 0.0292892855189821, validation loss: 0.23924263199923704\n",
      "Epoch 620, training loss: 0.02928892278369711, validation loss: 0.23923913987198706\n",
      "Epoch 630, training loss: 0.029288535539086547, validation loss: 0.23923559369728978\n",
      "Epoch 640, training loss: 0.029288124481390208, validation loss: 0.23923199520665367\n",
      "Epoch 650, training loss: 0.029287690289262893, validation loss: 0.23922834610717414\n",
      "Epoch 660, training loss: 0.029287233625369728, validation loss: 0.23922464808352903\n",
      "Epoch 670, training loss: 0.029286755137939584, validation loss: 0.23922090280008504\n",
      "Epoch 680, training loss: 0.02928625546227991, validation loss: 0.23921711190311007\n",
      "Epoch 690, training loss: 0.02928573522225519, validation loss: 0.23921327702308504\n",
      "Epoch 700, training loss: 0.029285195031731763, validation loss: 0.23920939977711242\n",
      "Epoch 710, training loss: 0.029284635495991012, validation loss: 0.23920548177141968\n",
      "Epoch 720, training loss: 0.02928405721311291, validation loss: 0.23920152460395258\n",
      "Epoch 730, training loss: 0.029283460775331515, validation loss: 0.23919752986706042\n",
      "Epoch 740, training loss: 0.029282846770364013, validation loss: 0.2391934991502685\n",
      "Epoch 750, training loss: 0.02928221578271431, validation loss: 0.23918943404314155\n",
      "Epoch 760, training loss: 0.02928156839495214, validation loss: 0.2391853361382327\n",
      "Epoch 770, training loss: 0.029280905188968517, validation loss: 0.23918120703412116\n",
      "Epoch 780, training loss: 0.029280226747207785, validation loss: 0.23917704833853778\n",
      "Epoch 790, training loss: 0.029279533653876602, validation loss: 0.2391728616715785\n",
      "Epoch 800, training loss: 0.02927882649612976, validation loss: 0.23916864866900436\n",
      "Epoch 810, training loss: 0.029278105865232645, validation loss: 0.23916441098563038\n",
      "Epoch 820, training loss: 0.029277372357699703, validation loss: 0.23916015029880047\n",
      "Epoch 830, training loss: 0.029276626576408366, validation loss: 0.2391558683119506\n",
      "Epoch 840, training loss: 0.02927586913168727, validation loss: 0.23915156675825655\n",
      "Epoch 850, training loss: 0.02927510064237784, validation loss: 0.2391472474043682\n",
      "Epoch 860, training loss: 0.029274321736867576, validation loss: 0.23914291205422677\n",
      "Epoch 870, training loss: 0.029273533054093706, validation loss: 0.23913856255296392\n",
      "Epoch 880, training loss: 0.029272735244515263, validation loss: 0.2391342007908809\n",
      "Epoch 890, training loss: 0.029271928971051568, validation loss: 0.23912982870750288\n",
      "Epoch 900, training loss: 0.029271114909985182, validation loss: 0.23912544829570723\n",
      "Epoch 910, training loss: 0.029270293751826628, validation loss: 0.239121061605919\n",
      "Epoch 920, training loss: 0.02926946620213881, validation loss: 0.23911667075036938\n",
      "Epoch 930, training loss: 0.02926863298231812, validation loss: 0.2391122779074107\n",
      "Epoch 940, training loss: 0.02926779483032968, validation loss: 0.23910788532588004\n",
      "Epoch 950, training loss: 0.02926695250139388, validation loss: 0.2391034953295042\n",
      "Epoch 960, training loss: 0.029266106768621007, validation loss: 0.23909911032133419\n",
      "Epoch 970, training loss: 0.029265258423591154, validation loss: 0.23909473278820037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 980, training loss: 0.029264408276876156, validation loss: 0.23909036530517436\n",
      "Epoch 990, training loss: 0.029263557158500503, validation loss: 0.23908601054002415\n",
      "Epoch 1000, training loss: 0.02926270591833799, validation loss: 0.23908167125764668\n",
      "Epoch 1010, training loss: 0.02926185542644095, validation loss: 0.23907735032446056\n",
      "Epoch 1020, training loss: 0.029261006573299084, validation loss: 0.23907305071274018\n",
      "Epoch 1030, training loss: 0.02926016027002464, validation loss: 0.2390687755048693\n",
      "Epoch 1040, training loss: 0.029259317448461067, validation loss: 0.23906452789749286\n",
      "Epoch 1050, training loss: 0.029258479061212282, validation loss: 0.2390603112055403\n",
      "Epoch 1060, training loss: 0.029257646081589737, validation loss: 0.23905612886609506\n",
      "Epoch 1070, training loss: 0.029256819503474776, validation loss: 0.23905198444208142\n",
      "Epoch 1080, training loss: 0.029256000341093803, validation loss: 0.2390478816257375\n",
      "Epoch 1090, training loss: 0.02925518962870421, validation loss: 0.2390438242418416\n",
      "Epoch 1100, training loss: 0.029254388420188942, validation loss: 0.23903981625065826\n",
      "Epoch 1110, training loss: 0.029253597788558065, validation loss: 0.23903586175056796\n",
      "Epoch 1120, training loss: 0.029252818825356075, validation loss: 0.2390319649803415\n",
      "Epoch 1130, training loss: 0.02925205263997355, validation loss: 0.2390281303210216\n",
      "Epoch 1140, training loss: 0.029251300358862642, validation loss: 0.23902436229737017\n",
      "Epoch 1150, training loss: 0.029250563124655755, validation loss: 0.2390206655788407\n",
      "Epoch 1160, training loss: 0.02924984209518751, validation loss: 0.23901704498003393\n",
      "Epoch 1170, training loss: 0.029249138442420174, validation loss: 0.2390135054605944\n",
      "Epoch 1180, training loss: 0.029248453351273396, validation loss: 0.239010052124507\n",
      "Epoch 1190, training loss: 0.0292477880183591, validation loss: 0.2390066902187499\n",
      "Epoch 1200, training loss: 0.029247143650623433, validation loss: 0.2390034251312671\n",
      "Epoch 1210, training loss: 0.029246521463897515, validation loss: 0.23900026238821806\n",
      "Epoch 1220, training loss: 0.029245922681359327, validation loss: 0.23899720765047075\n",
      "Epoch 1230, training loss: 0.029245348531909788, validation loss: 0.23899426670930177\n",
      "Epoch 1240, training loss: 0.02924480024846626, validation loss: 0.23899144548127546\n",
      "Epoch 1250, training loss: 0.029244279066177076, validation loss: 0.2389887500022713\n",
      "Epoch 1260, training loss: 0.029243786220561535, validation loss: 0.23898618642064126\n",
      "Epoch 1270, training loss: 0.029243322945579804, validation loss: 0.2389837609894754\n",
      "Epoch 1280, training loss: 0.02924289047163794, validation loss: 0.2389814800579675\n",
      "Epoch 1290, training loss: 0.02924249002353355, validation loss: 0.2389793500618713\n",
      "Epoch 1300, training loss: 0.02924212281834787, validation loss: 0.23897737751305126\n",
      "Epoch 1310, training loss: 0.02924179006329097, validation loss: 0.23897556898813285\n",
      "Epoch 1320, training loss: 0.029241492953506407, validation loss: 0.2389739311162696\n",
      "Epoch 1330, training loss: 0.029241232669842854, validation loss: 0.238972470566049\n",
      "Epoch 1340, training loss: 0.029241010376599926, validation loss: 0.23897119403156825\n",
      "Epoch 1350, training loss: 0.029240827219256235, validation loss: 0.23897010821771947\n",
      "Epoch 1360, training loss: 0.029240684322187613, validation loss: 0.23896921982473165\n",
      "Epoch 1370, training loss: 0.029240582786384195, validation loss: 0.23896853553202535\n",
      "Epoch 1380, training loss: 0.02924052368717475, validation loss: 0.23896806198144394\n",
      "Epoch 1390, training loss: 0.029240508071967337, validation loss: 0.2389678057599318\n",
      "Epoch 1400, training loss: 0.0292405369580153, validation loss: 0.2389677733817407\n",
      "Epoch 1410, training loss: 0.029240611330217717, validation loss: 0.2389679712702479\n",
      "Epoch 1420, training loss: 0.029240732138963714, validation loss: 0.2389684057394813\n",
      "Epoch 1430, training loss: 0.02924090029802982, validation loss: 0.23896908297544597\n",
      "Epoch 1440, training loss: 0.029241116682539807, validation loss: 0.23897000901735882\n",
      "Epoch 1450, training loss: 0.02924138212699607, validation loss: 0.23897118973889536\n",
      "Epoch 1460, training loss: 0.02924169742339188, validation loss: 0.2389726308295611\n",
      "Epoch 1470, training loss: 0.02924206331941322, validation loss: 0.23897433777629729\n",
      "Epoch 1480, training loss: 0.029242480516739142, validation loss: 0.23897631584543683\n",
      "Epoch 1490, training loss: 0.029242949669448723, validation loss: 0.23897857006512097\n",
      "Epoch 1500, training loss: 0.029243471382543008, validation loss: 0.23898110520829047\n",
      "Epoch 1510, training loss: 0.02924404621058927, validation loss: 0.23898392577636013\n",
      "Epoch 1520, training loss: 0.029244674656494753, validation loss: 0.238987035983682\n",
      "Epoch 1530, training loss: 0.02924535717041672, validation loss: 0.23899043974290113\n",
      "Epoch 1540, training loss: 0.029246094148814333, validation loss: 0.2389941406512964\n",
      "Epoch 1550, training loss: 0.029246885933648138, validation loss: 0.23899814197819957\n",
      "Epoch 1560, training loss: 0.029247732811731438, validation loss: 0.23900244665357182\n",
      "Epoch 1570, training loss: 0.029248635014237526, validation loss: 0.23900705725781204\n",
      "Epoch 1580, training loss: 0.029249592716365846, validation loss: 0.23901197601286245\n",
      "Epoch 1590, training loss: 0.029250606037169165, validation loss: 0.2390172047746648\n",
      "Epoch 1600, training loss: 0.0292516750395432, validation loss: 0.23902274502701154\n",
      "Epoch 1610, training loss: 0.029252799730379135, validation loss: 0.23902859787682884\n",
      "Epoch 1620, training loss: 0.02925398006087832, validation loss: 0.23903476405090957\n",
      "Epoch 1630, training loss: 0.029255215927028087, validation loss: 0.2390412438941118\n",
      "Epoch 1640, training loss: 0.02925650717023601, validation loss: 0.23904803736902194\n",
      "Epoch 1650, training loss: 0.029257853578119546, validation loss: 0.23905514405707187\n",
      "Epoch 1660, training loss: 0.02925925488544669, validation loss: 0.23906256316108837\n",
      "Epoch 1670, training loss: 0.029260710775222763, validation loss: 0.23907029350924433\n",
      "Epoch 1680, training loss: 0.02926222087991716, validation loss: 0.23907833356036715\n",
      "Epoch 1690, training loss: 0.02926378478282346, validation loss: 0.23908668141055472\n",
      "Epoch 1700, training loss: 0.029265402019545295, validation loss: 0.23909533480103648\n",
      "Epoch 1710, training loss: 0.029267072079599606, validation loss: 0.23910429112721093\n",
      "Epoch 1720, training loss: 0.02926879440812844, validation loss: 0.23911354744878305\n",
      "Epoch 1730, training loss: 0.02927056840770967, validation loss: 0.23912310050091712\n",
      "Epoch 1740, training loss: 0.029272393440256617, validation loss: 0.23913294670631596\n",
      "Epoch 1750, training loss: 0.029274268828995857, validation loss: 0.2391430821881327\n",
      "Epoch 1760, training loss: 0.029276193860512473, validation loss: 0.23915350278361544\n",
      "Epoch 1770, training loss: 0.029278167786851307, validation loss: 0.23916420405838346\n",
      "Epoch 1780, training loss: 0.02928018982766301, validation loss: 0.23917518132123208\n",
      "Epoch 1790, training loss: 0.02928225917238298, validation loss: 0.23918642963935993\n",
      "Epoch 1800, training loss: 0.029284374982431825, validation loss: 0.239197943853914\n",
      "Epoch 1810, training loss: 0.029286536393425565, validation loss: 0.23920971859574722\n",
      "Epoch 1820, training loss: 0.029288742517384066, validation loss: 0.2392217483012855\n",
      "Epoch 1830, training loss: 0.029290992444926625, validation loss: 0.23923402722840353\n",
      "Epoch 1840, training loss: 0.029293285247443245, validation loss: 0.23924654947220902\n",
      "Epoch 1850, training loss: 0.029295619979231337, validation loss: 0.23925930898064318\n",
      "Epoch 1860, training loss: 0.029297995679587165, validation loss: 0.23927229956980536\n",
      "Epoch 1870, training loss: 0.029300411374842465, validation loss: 0.23928551493891675\n",
      "Epoch 1880, training loss: 0.029302866080336748, validation loss: 0.239298948684842\n",
      "Epoch 1890, training loss: 0.029305358802316554, validation loss: 0.2393125943160952\n",
      "Epoch 1900, training loss: 0.029307888539753607, validation loss: 0.23932644526625968\n",
      "Epoch 1910, training loss: 0.029310454286074216, validation loss: 0.23934049490676057\n",
      "Epoch 1920, training loss: 0.02931305503079326, validation loss: 0.23935473655893313\n",
      "Epoch 1930, training loss: 0.029315689761046548, validation loss: 0.239369163505338\n",
      "Epoch 1940, training loss: 0.029318357463016146, validation loss: 0.23938376900027936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1950, training loss: 0.029321057123244197, validation loss: 0.23939854627949217\n",
      "Epoch 1960, training loss: 0.029323787729831085, validation loss: 0.2394134885689673\n",
      "Epoch 1970, training loss: 0.029326548273515024, validation loss: 0.23942858909289208\n",
      "Epoch 1980, training loss: 0.029329337748630408, validation loss: 0.23944384108069156\n",
      "Epoch 1990, training loss: 0.029332155153943645, validation loss: 0.2394592377731591\n"
     ]
    }
   ],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve inputs and targets at the given index\n",
    "        X = self.inputs[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "validSet=Dataset(ipt, opt)\n",
    "\n",
    "trainSet=Dataset(ipt, opt)\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 2000\n",
    "\n",
    "# Initialize a new network\n",
    "hd=50\n",
    "z_size = hd + numFeats # Size of concatenated hidden + input vector\n",
    "params = init_lstm(hd, numFeats, z=z_size)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hd, 1))\n",
    "\n",
    "# Track loss\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "# For each epoch\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "\n",
    "    # For each sentence in validation set\n",
    "    for inputs, targets in validSet:\n",
    "        \n",
    "        # Initialize hidden state and cell state as zeros\n",
    "        h = np.zeros((hd, 1))\n",
    "        c = np.zeros((hd, 1))\n",
    "\n",
    "        # Forward pass\n",
    "        zs, fs, ins, gs, Cs, os, hs, vs, outputs = forward(inputs, h, c, params)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss, _ = backward(zs, fs, ins, gs, Cs, os, hs, vs, outputs, targets, params)\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_validation_loss += loss\n",
    "    \n",
    "    # For each sentence in training set\n",
    "    for inputs, targets in trainSet:\n",
    "        \n",
    "        # Initialize hidden state and cell state as zeros\n",
    "        h = np.zeros((hd, 1))\n",
    "        c = np.zeros((hd, 1))\n",
    "\n",
    "        # Forward pass\n",
    "        zs, fs, ins, gs, Cs, os, hs, vs, outputs = forward(inputs, h, c, params)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss, grads = backward(zs, fs, ins, gs, Cs, os, hs, vs, outputs, targets, params)\n",
    "        \n",
    "        # Update parameters\n",
    "        params = update_parameters(params, grads, lr=1e-1)\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_training_loss += loss\n",
    "                \n",
    "    # Save loss for plot\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "feca29a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T08:23:05.786641Z",
     "start_time": "2021-11-23T08:23:05.766466Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 10, got 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-242-5a7a915e8602>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mzs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mipt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 10, got 9)"
     ]
    }
   ],
   "source": [
    "zs, fs, ins, gs, Cs, os, hs, vs, outputs, outputs = forward(ipt[1], h, c, params)\n",
    "print(v_s)\n",
    "print(opt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f64dfd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
