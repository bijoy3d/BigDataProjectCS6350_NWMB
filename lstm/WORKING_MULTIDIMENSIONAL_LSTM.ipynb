{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef77dc2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T06:01:09.323160Z",
     "start_time": "2021-11-23T06:01:08.826469Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3a7b285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T06:21:52.344490Z",
     "start_time": "2021-11-23T06:21:52.325113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [1]]\n",
      "--\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "dataset=StringIO(\"\"\"Date,Open,High,Low,Close,Volume,Trade_count,Vwap\n",
    "2015-12-01 09:00:00+00:00,118.88,118.94,118.88,118.94,1145,5,118.902052\n",
    "2015-12-01 09:15:00+00:00,118.77,118.77,118.77,118.77,200,1,118.77\n",
    "2015-12-01 09:30:00+00:00,118.69,118.69,118.6,118.6,900,4,118.61\n",
    "2015-12-01 09:45:00+00:00,118.64,118.65,118.64,118.65,3580,5,118.648883\n",
    "2015-12-01 10:00:00+00:00,118.65,118.65,118.55,118.55,1820,4,118.611538\n",
    "2015-12-01 10:15:00+00:00,118.55,118.6,118.55,118.6,880,5,118.5625\n",
    "2015-12-01 10:30:00+00:00,118.55,118.55,118.5,118.5,1878,5,118.513312\n",
    "2015-12-01 10:45:00+00:00,118.59,118.72,118.59,118.72,2499,10,118.628431\n",
    "2015-12-01 11:00:00+00:00,118.71,118.9,118.71,118.9,2842,11,118.86064\n",
    "2015-12-01 11:15:00+00:00,118.87,118.87,118.87,118.87,300,2,118.87\n",
    "2015-12-01 11:30:00+00:00,118.78,118.8,118.76,118.8,3914,22,118.785876\n",
    "2015-12-01 11:45:00+00:00,118.8,118.99,118.77,118.9,7900,37,118.893542\n",
    "2015-12-01 12:00:00+00:00,118.88,118.98,118.84,118.84,6540,34,118.922648\n",
    "2015-12-01 12:15:00+00:00,118.82,118.84,118.77,118.77,5603,28,118.804962\n",
    "2015-12-01 12:30:00+00:00,118.77,118.89,118.76,118.88,7612,31,118.824002\n",
    "\"\"\")\n",
    "df = pd.read_table(dataset, sep=\",\")\n",
    "\n",
    "#ip = np.array([ [1,2,3],[6,8,9],[3,4,5],[4,7,8],[4,2,5],[5,7,4] ])\n",
    "#op = np.array([[2,8,4,7,2,4]])\n",
    "#op = op.reshape(6,1)\n",
    "ip = np.array([ [1],  [2],  [0],  [2],  [0],  [1],  [2],  [1] ])\n",
    "op = np.array([ [300],[100],[200],[100],[200],[300],[100],[300] ])\n",
    "num_steps = 3\n",
    "num_features = 3\n",
    "#ip_shaped = np.reshape(ip, newshape=(-1, num_steps, num_features))\n",
    "\n",
    "#X = np.array([ [1,2,3, 4, 5, 6] ])\n",
    "#Y = np.array([[2,3,4,5,6,7]])\n",
    "print(ip)\n",
    "print(\"--\")\n",
    "#ip= np.tile(ip,(50,1))\n",
    "#op = np.tile(op,(50,1))\n",
    "print(len(op))\n",
    "#ip = np.array([ [1],[1],[1],[1],[1],[2],[1],[2]])\n",
    "#op = np.array([ [1,    0,    1,    0,    1,    1,    0,    1 ]]).T\n",
    "\n",
    "ip = np.array([ [1,1,2],[1,2,1],[1,0,2],[1,2,1],[1,0,2],[1,1,2],[1,2,1],[1,1,2]])\n",
    "op = np.array([ [1,    0,    1,    0,    1,    1,    0,    1 ]]).T\n",
    "#ip = np.array([ [1,1,1,1,1],[0,2,2,2,2],[1,3,3,3,3],[1,4,4,4,4],[0,5,5,5,5],[0,6,6,6,6],[1,7,7,7,7],[0,8,8,8,8]])\n",
    "#op = np.array([ [1,          0,          1,          1,          0,          0,          1,          0 ]]).T\n",
    "_, numFeats = ip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1581704c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T07:34:13.166751Z",
     "start_time": "2021-11-23T07:34:13.152527Z"
    }
   },
   "outputs": [],
   "source": [
    "def lstm_data_transform(x_data, y_data, timeSteps=2):\n",
    "    \"\"\" Changes data to the format for LSTM training \n",
    "for sliding window approach \"\"\"\n",
    "    # Prepare the list for the transformed data\n",
    "    X, y = list(), list()\n",
    "    # Loop of the entire data set\n",
    "    #print(x_data.shape[0])\n",
    "    for i in range(x_data.shape[0]):\n",
    "        # compute a new (sliding window) index\n",
    "        end_ix = i + timeSteps\n",
    "\n",
    "        # if index is larger than the size of the dataset, we stop\n",
    "        #print(end_ix)\n",
    "        if end_ix >= x_data.shape[0]:\n",
    "            break\n",
    "        # Get a sequence of data for x\n",
    "        seq_X = x_data[i:end_ix]\n",
    "        #print(x_data[i:end_ix])\n",
    "        # Get only the last element of the sequency for y\n",
    "        #print(y_data[end_ix])\n",
    "        seq_y = y_data[i:end_ix]\n",
    "        # Append the list with sequencies\n",
    "        X.append(seq_X)\n",
    "        y.append(seq_y)\n",
    "        display(X)\n",
    "        print(\"--\")\n",
    "    # Make final arrays\n",
    "    x_array = np.array(X)\n",
    "    y_array = np.array(y)\n",
    "    return x_array, y_array\n",
    "\n",
    "def lstm_data_time(x_data, y_data, timeSteps=2):\n",
    "    \"\"\" Changes data to the format for LSTM training \n",
    "for sliding window approach \"\"\"\n",
    "    # Prepare the list for the transformed data\n",
    "    X, y = list(), list()\n",
    "    # Loop of the entire data set\n",
    "    #print(x_data.shape[0])\n",
    "    x_array = []\n",
    "    y_array = []\n",
    "\n",
    "    for i in range(x_data.shape[0]):\n",
    "        my2, mx2 = list(), list()\n",
    "        # compute a new (sliding window) index\n",
    "        end_ix = i + timeSteps\n",
    "\n",
    "        # if index is larger than the size of the dataset, we stop\n",
    "        #print(end_ix)\n",
    "        if end_ix >= x_data.shape[0]:\n",
    "            break\n",
    "\n",
    "        for j in range(i, i+timeSteps):\n",
    "            my1, mx1 = list(), list()\n",
    "            # Get a sequence of data for x\n",
    "            seq_X = x_data[j]\n",
    "            # Get only the last element of the sequency for y\n",
    "            seq_y = y_data[j]\n",
    "            # Append the list with sequencies\n",
    "            for k in range(len(seq_X)):\n",
    "                my, mx = list(), list()\n",
    "                mx.append(seq_X[k])\n",
    "                mx1.append(mx)\n",
    "                \n",
    "            mx2.append(mx1)\n",
    "            my2.append(seq_y)\n",
    "            \n",
    "        X.append(mx2)\n",
    "        y.append(my2)\n",
    "\n",
    "        #X.append(np.array(mx1))\n",
    "        #y.append(np.array(my))\n",
    "        print(\"--\")\n",
    "    # Make final arrays\n",
    "    x_array = np.array(X)\n",
    "    y_array = np.array(y)\n",
    "\n",
    "    return x_array, y_array\n",
    "#3 3 1 want\n",
    "#3 3 3 now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8de04139",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T07:45:29.252456Z",
     "start_time": "2021-11-23T07:45:29.240214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4)\n",
      "--\n",
      "--\n",
      "--\n",
      "--\n",
      "--\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [2]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [2],\n",
       "         [1]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [2]]],\n",
       "\n",
       "\n",
       "       [[[1],\n",
       "         [1],\n",
       "         [2],\n",
       "         [1]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [2]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [2],\n",
       "         [1]]],\n",
       "\n",
       "\n",
       "       [[[1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [2]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [2],\n",
       "         [1]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [2]]],\n",
       "\n",
       "\n",
       "       [[[1],\n",
       "         [1],\n",
       "         [2],\n",
       "         [1]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [2]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [2]]],\n",
       "\n",
       "\n",
       "       [[[1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [2]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [2]],\n",
       "\n",
       "        [[1],\n",
       "         [1],\n",
       "         [2],\n",
       "         [1]]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3, 4, 1)\n",
      "(3, 4, 1)\n",
      "[[[1]\n",
      "  [1]\n",
      "  [1]\n",
      "  [2]]\n",
      "\n",
      " [[1]\n",
      "  [1]\n",
      "  [2]\n",
      "  [1]]\n",
      "\n",
      " [[1]\n",
      "  [1]\n",
      "  [0]\n",
      "  [2]]]\n",
      "(5, 3, 1)\n",
      "====\n",
      "[[1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "timeSteps = 3\n",
    "ip = np.array([ [1,1,1,2],[1,1,2,1],[1,1,0,2],[1,1,2,1],[1,1,0,2],[1,1,1,2],[1,1,2,1],[1,1,1,2]])\n",
    "print(ip.shape)\n",
    "ipt,opt=lstm_data_time(ip,op, timeSteps)\n",
    "display(ipt)\n",
    "print(ipt.shape)\n",
    "print(ipt[0].shape)\n",
    "print(ipt[0])\n",
    "print(opt.shape)\n",
    "\n",
    "print(\"====\")\n",
    "print(opt[0])\n",
    "# batchSize = len(ipt)\n",
    "# [[[1.]\n",
    "#   [0.]\n",
    "#   [0.]]\n",
    "\n",
    "#  [[1.]\n",
    "#   [0.]\n",
    "#   [0.]]\n",
    "\n",
    "#  [[1.]\n",
    "#   [0.]\n",
    "#   [0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9912c5b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T06:01:39.169544Z",
     "start_time": "2021-11-23T06:01:39.151629Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the element-wise sigmoid activation function for an array x.\n",
    "\n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    f = 1 / (1 + np.exp(-x_safe))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        return f * (1 - f)\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f\n",
    "\n",
    "\n",
    "\n",
    "def init_orthogonal(param):\n",
    "    \"\"\"\n",
    "    Initializes weight parameters orthogonally.\n",
    "    This is a common initiailization for recurrent neural networks.\n",
    "    \n",
    "    Refer to this paper for an explanation of this initialization:\n",
    "    https://arxiv.org/abs/1312.6120\n",
    "    \"\"\"\n",
    "    if param.ndim < 2:\n",
    "        raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
    "\n",
    "    rows, cols = param.shape\n",
    "    \n",
    "    new_param = np.random.randn(rows, cols)\n",
    "    \n",
    "    if rows < cols:\n",
    "        new_param = new_param.T\n",
    "    \n",
    "    # Compute QR factorization\n",
    "    q, r = np.linalg.qr(new_param)\n",
    "    \n",
    "    # Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\n",
    "    d = np.diag(r, 0)\n",
    "    ph = np.sign(d)\n",
    "    q *= ph\n",
    "\n",
    "    if rows < cols:\n",
    "        q = q.T\n",
    "    \n",
    "    new_param = q\n",
    "    \n",
    "    return new_param\n",
    "\n",
    "  \n",
    "def tanh(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the element-wise tanh activation function for an array x.\n",
    "\n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    f = (np.exp(x_safe)-np.exp(-x_safe))/(np.exp(x_safe)+np.exp(-x_safe))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        return 1-f**2\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f\n",
    "    \n",
    "    \n",
    "def softmax(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the softmax for an array x.\n",
    "    \n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    f = np.exp(x_safe) / np.sum(np.exp(x_safe))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        pass # We will not need this one\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f\n",
    "    \n",
    "def clip_gradient_norm(grads, max_norm=0.25):\n",
    "    \"\"\"\n",
    "    Clips gradients to have a maximum norm of `max_norm`.\n",
    "    This is to prevent the exploding gradients problem.\n",
    "    \"\"\" \n",
    "    # Set the maximum of the norm to be of type float\n",
    "    max_norm = float(max_norm)\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculate the L2 norm squared for each gradient and add them to the total norm\n",
    "    for grad in grads:\n",
    "        grad_norm = np.sum(np.power(grad, 2))\n",
    "        total_norm += grad_norm\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Calculate clipping coeficient\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    \n",
    "    # If the total norm is larger than the maximum allowable norm, then clip the gradient\n",
    "    if clip_coef < 1:\n",
    "        for grad in grads:\n",
    "            grad *= clip_coef\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def update_parameters(params, grads, lr=1e-3):\n",
    "    # Take a step\n",
    "    for param, grad in zip(params, grads):\n",
    "        param -= lr * grad\n",
    "    \n",
    "    return params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c8c5e9d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T07:45:48.904987Z",
     "start_time": "2021-11-23T07:45:48.882137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wxi: (1, 3)\n",
      "wxf: (1, 3)\n",
      "wxc: (1, 3)\n",
      "wxo: (1, 3)\n",
      "whi: (6, 3)\n",
      "whf: (6, 3)\n",
      "whc: (6, 3)\n",
      "who: (6, 3)\n",
      "wy: (6, 1)\n",
      "bi: (3, 1)\n",
      "bf: (6, 1)\n",
      "bc: (6, 1)\n",
      "bo: (6, 1)\n",
      "by: (6, 1)\n"
     ]
    }
   ],
   "source": [
    "od=1\n",
    "def init_lstm(hd, timeSteps):\n",
    "    \"\"\"\n",
    "    Initializes our LSTM network.\n",
    "    \n",
    "    Args:\n",
    "     `hidden_size`: the dimensions of the hidden state\n",
    "     `vocab_size`: the dimensions of our vocabulary\n",
    "     `z_size`: the dimensions of the concatenated input \n",
    "    \"\"\"\n",
    "    z=hd+timeSteps\n",
    "    \n",
    "    wxi = 2*np.random.random((1,numFeats)) - 1\n",
    "    wxf = 2*np.random.random((1,numFeats)) - 1\n",
    "    wxc = 2*np.random.random((1,numFeats)) - 1\n",
    "    wxo = 2*np.random.random((1,numFeats)) - 1\n",
    "\n",
    "    whi= 2*np.random.random((hd,numFeats)) - 1\n",
    "    whf= 2*np.random.random((hd,numFeats)) - 1\n",
    "    whc= 2*np.random.random((hd,numFeats)) - 1\n",
    "    who = 2*np.random.random((hd,numFeats)) - 1\n",
    "\n",
    "    wy = 2*np.random.random((hd,1)) - 1\n",
    "\n",
    "    \n",
    "    bi = np.zeros((numFeats, 1))\n",
    "    bf = np.zeros((hd, 1))\n",
    "    bc = np.zeros((hd, 1))\n",
    "    bo = np.zeros((hd, 1))\n",
    "    \n",
    "    by = np.zeros((hd, 1))\n",
    "    \n",
    "\n",
    "    return wxi, wxf, wxc, wxo, whi, whf, whc, who, wy, bi, bf, bc, bo, by\n",
    "\n",
    "\n",
    "params = init_lstm(6, timeSteps)\n",
    "print('wxi:', params[0].shape)\n",
    "print('wxf:', params[1].shape)\n",
    "print('wxc:', params[2].shape)\n",
    "print('wxo:', params[3].shape)\n",
    "\n",
    "print('whi:', params[4].shape)\n",
    "print('whf:', params[5].shape)\n",
    "print('whc:', params[6].shape)\n",
    "print('who:', params[7].shape)\n",
    "\n",
    "print('wy:', params[8].shape)\n",
    "\n",
    "print('bi:', params[9].shape)\n",
    "print('bf:', params[10].shape)\n",
    "print('bc:', params[11].shape)\n",
    "print('bo:', params[12].shape)\n",
    "print('by:', params[13].shape)\n",
    "\n",
    "for param in params:\n",
    "    assert param.ndim == 2, \\\n",
    "        'all parameters should be 2-dimensional '\\\n",
    "        '(hint: a dimension can simply have size 1)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfcd3cb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T06:01:42.132582Z",
     "start_time": "2021-11-23T06:01:42.120953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A single sample from the generated dataset:\n",
      "['1', '1', '1', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set seed such that we always get the same dataset\n",
    "# (this is a good idea in general)\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_dataset(num_sequences=2**8):\n",
    "    \"\"\"\n",
    "    Generates a number of sequences as our dataset.\n",
    "    \n",
    "    Args:\n",
    "     `num_sequences`: the number of sequences to be generated.\n",
    "     \n",
    "    Returns a list of sequences.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    for _ in range(num_sequences): \n",
    "        num_tokens = np.random.randint(1, 12)\n",
    "        sample = ['1'] * num_tokens + ['0'] * num_tokens\n",
    "        samples.append(sample)\n",
    "        \n",
    "    return samples\n",
    "\n",
    "\n",
    "sequences = generate_dataset()\n",
    "\n",
    "print('A single sample from the generated dataset:')\n",
    "print(sequences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e85bad2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T06:01:43.592647Z",
     "start_time": "2021-11-23T06:01:43.583124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 256 sentences and 3 unique tokens in our dataset (including UNK).\n",
      "\n",
      "The index of '1' is 0\n",
      "The word corresponding to index 0 is '1'\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def sequences_to_dicts(sequences):\n",
    "    \"\"\"\n",
    "    Creates word_to_idx and idx_to_word dictionaries for a list of sequences.\n",
    "    \"\"\"\n",
    "    # A bit of Python-magic to flatten a nested list\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    \n",
    "    # Flatten the dataset\n",
    "    all_words = flatten(sequences)\n",
    "    \n",
    "    # Count number of word occurences\n",
    "    word_count = defaultdict(int)\n",
    "    for word in flatten(sequences):\n",
    "        word_count[word] += 1\n",
    "\n",
    "    # Sort by frequency\n",
    "    word_count = sorted(list(word_count.items()), key=lambda l: -l[1])\n",
    "\n",
    "    # Create a list of all unique words\n",
    "    unique_words = [item[0] for item in word_count]\n",
    "\n",
    "    # Add UNK token to list of words\n",
    "    unique_words.append('UNK')\n",
    "\n",
    "    # Count number of sequences and number of unique words\n",
    "    num_sentences, vocab_size = len(sequences), len(unique_words)\n",
    "\n",
    "    # Create dictionaries so that we can go from word to index and back\n",
    "    # If a word is not in our vocabulary, we assign it to token 'UNK'\n",
    "    word_to_idx = defaultdict(lambda: vocab_size-1)\n",
    "    idx_to_word = defaultdict(lambda: 'UNK')\n",
    "\n",
    "    # Fill dictionaries\n",
    "    for idx, word in enumerate(unique_words):\n",
    "        word_to_idx[word] = idx \n",
    "        idx_to_word[idx] = word\n",
    "\n",
    "    return word_to_idx, idx_to_word, num_sentences, vocab_size\n",
    "\n",
    "\n",
    "word_to_idx, idx_to_word, num_sequences, vocab_size = sequences_to_dicts(sequences)\n",
    "\n",
    "print(f'We have {num_sequences} sentences and {len(word_to_idx)} unique tokens in our dataset (including UNK).\\n')\n",
    "print('The index of \\'1\\' is', word_to_idx['1'])\n",
    "print(f'The word corresponding to index 0 is \\'{idx_to_word[0]}\\'')\n",
    "\n",
    "assert idx_to_word[word_to_idx['1']] == '1', \\\n",
    "    'Consistency error: something went wrong in the conversion.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4745f452",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T06:01:44.380926Z",
     "start_time": "2021-11-23T06:01:44.373216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our one-hot encoding of '1' has shape [1. 0. 0.].\n",
      "Our one-hot encoding of '1 0' has shape [[[1.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [1.]\n",
      "  [0.]]].\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(idx, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a single word given its index and the size of the vocabulary.\n",
    "    \n",
    "    Args:\n",
    "     `idx`: the index of the given word\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "    \n",
    "    Returns a 1-D numpy array of length `vocab_size`.\n",
    "    \"\"\"\n",
    "    # Initialize the encoded array\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    \n",
    "    # Set the appropriate element to one\n",
    "    one_hot[idx] = 1.0\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def one_hot_encode_sequence(sequence, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a sequence of words given a fixed vocabulary size.\n",
    "    \n",
    "    Args:\n",
    "     `sentence`: a list of words to encode\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "     \n",
    "    Returns a 3-D numpy array of shape (num words, vocab size, 1).\n",
    "    \"\"\"\n",
    "    # Encode each word in the sentence\n",
    "    encoding = np.array([one_hot_encode(word_to_idx[word], vocab_size) for word in sequence])\n",
    "\n",
    "    # Reshape encoding s.t. it has shape (num words, vocab size, 1)\n",
    "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1)\n",
    "    \n",
    "    return encoding\n",
    "\n",
    "\n",
    "test_word = one_hot_encode(word_to_idx['1'], vocab_size)\n",
    "print(f'Our one-hot encoding of \\'1\\' has shape {test_word}.')\n",
    "\n",
    "test_sentence = one_hot_encode_sequence(['1', '0'], vocab_size)\n",
    "print(f'Our one-hot encoding of \\'1 0\\' has shape {test_sentence}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0806f9a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T07:55:41.264978Z",
     "start_time": "2021-11-23T07:55:41.247333Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25,)\n",
      "We have 204 samples in the training set.\n",
      "We have 25 samples in the validation set.\n",
      "We have 25 samples in the test set.\n",
      "['1', '1', '1', '1', '1', '1', '0', '0', '0', '0', '0'] ['1', '1', '1', '1', '1', '0', '0', '0', '0', '0', '0']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve inputs and targets at the given index\n",
    "        X = self.inputs[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "def create_datasets(sequences, dataset_class, p_train=0.8, p_val=0.1, p_test=0.1):\n",
    "    # Define partition sizes\n",
    "    num_train = int(len(sequences)*p_train)\n",
    "    num_val = int(len(sequences)*p_val)\n",
    "    num_test = int(len(sequences)*p_test)\n",
    "\n",
    "    # Split sequences into partitions\n",
    "    sequences_train = sequences[:num_train]\n",
    "    sequences_val = sequences[num_train:num_train+num_val]\n",
    "    sequences_test = sequences[-num_test:]\n",
    "\n",
    "    def get_inputs_targets_from_sequences(sequences):\n",
    "        # Define empty lists\n",
    "        inputs, targets = [], []\n",
    "        \n",
    "        # Append inputs and targets s.t. both lists contain L-1 words of a sentence of length L\n",
    "        # but targets are shifted right by one so that we can predict the next word\n",
    "        for sequence in sequences:\n",
    "            inputs.append(sequence[:-1])\n",
    "            targets.append(sequence[1:])\n",
    "            \n",
    "        return inputs, targets\n",
    "\n",
    "    # Get inputs and targets for each partition\n",
    "    inputs_train, targets_train = get_inputs_targets_from_sequences(sequences_train)\n",
    "    inputs_val, targets_val = get_inputs_targets_from_sequences(sequences_val)\n",
    "    inputs_test, targets_test = get_inputs_targets_from_sequences(sequences_test)\n",
    "    print(np.array(inputs_val).shape)\n",
    "\n",
    "    # Create datasets\n",
    "    training_set = dataset_class(inputs_train, targets_train)\n",
    "    validation_set = dataset_class(inputs_val, targets_val)\n",
    "    test_set = dataset_class(inputs_test, targets_test)\n",
    "\n",
    "    return training_set, validation_set, test_set\n",
    "    \n",
    "\n",
    "training_set, validation_set, test_set = create_datasets(sequences, Dataset)\n",
    "\n",
    "print(f'We have {len(training_set)} samples in the training set.')\n",
    "print(f'We have {len(validation_set)} samples in the validation set.')\n",
    "print(f'We have {len(test_set)} samples in the test set.')\n",
    "for inputs, targets in validation_set:\n",
    "        print(inputs, targets)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7870a92d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T07:46:06.835709Z",
     "start_time": "2021-11-23T07:46:06.818738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_f: (50, 54)\n",
      "W_i: (50, 54)\n",
      "W_g: (50, 54)\n",
      "W_o: (50, 54)\n",
      "W_v: (1, 50)\n",
      "b_i: (50, 1)\n",
      "b_g: (50, 1)\n",
      "b_o: (50, 1)\n",
      "b_v: (50, 1)\n"
     ]
    }
   ],
   "source": [
    "# Size of concatenated hidden + input vector\n",
    "hd = 50\n",
    "numFeats = ind = 4\n",
    "z = hd + ind\n",
    "\n",
    "def init_lstm(hd, ind, z):\n",
    "    \"\"\"\n",
    "    Initializes our LSTM network.\n",
    "    \n",
    "    Args:\n",
    "     `hidden_size`: the dimensions of the hidden state\n",
    "     `vocab_size`: the dimensions of our vocabulary\n",
    "     `z_size`: the dimensions of the concatenated input \n",
    "    \"\"\"\n",
    "    # Weight matrix (forget gate)\n",
    "    W_f = np.zeros((hd, z))\n",
    "   \n",
    "    # Bias for forget gate\n",
    "    b_f = np.zeros((hd, 1))\n",
    "\n",
    "    # Weight matrix (input gate)\n",
    "    W_i = np.zeros((hd, z))\n",
    "    \n",
    "    # Bias for input gate\n",
    "    b_i = np.zeros((hd, 1))\n",
    "\n",
    "    # Weight matrix (candidate)\n",
    "    W_g = np.zeros((hd, z))\n",
    "\n",
    "    # Bias for candidate\n",
    "    b_g = np.zeros((hd, 1))\n",
    "\n",
    "    # Weight matrix of the output gate\n",
    "    W_o = np.zeros((hd, z))\n",
    "    \n",
    "    # Bias for output gate\n",
    "    b_o = np.zeros((hd, 1))\n",
    "\n",
    "    # Weight matrix relating the hidden-state to the output\n",
    "    W_v = np.zeros((1, hd))\n",
    "    \n",
    "    # Bias for logits\n",
    "    b_v = np.zeros((1, 1))\n",
    "    \n",
    "    # Initialize weights according to https://arxiv.org/abs/1312.6120\n",
    "    W_f = init_orthogonal(W_f)\n",
    "    W_i = init_orthogonal(W_i)\n",
    "    W_g = init_orthogonal(W_g)\n",
    "    W_o = init_orthogonal(W_o)\n",
    "    W_v = init_orthogonal(W_v)\n",
    "\n",
    "    return W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v\n",
    "\n",
    "\n",
    "params = init_lstm(hd, ind, z)\n",
    "print('W_f:', params[0].shape)\n",
    "print('W_i:', params[1].shape)\n",
    "print('W_g:', params[2].shape)\n",
    "print('W_o:', params[3].shape)\n",
    "print('W_v:', params[4].shape)\n",
    "print('b_i:', params[5].shape)\n",
    "print('b_g:', params[6].shape)\n",
    "print('b_o:', params[7].shape)\n",
    "print('b_v:', params[8].shape)\n",
    "\n",
    "for param in params:\n",
    "    assert param.ndim == 2, \\\n",
    "        'all parameters should be 2-dimensional '\\\n",
    "        '(hint: a dimension can simply have size 1)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "63f18e3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T08:12:19.109939Z",
     "start_time": "2021-11-23T08:12:19.087669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence:\n",
      "[[[1]\n",
      "  [1]\n",
      "  [1]\n",
      "  [2]]\n",
      "\n",
      " [[1]\n",
      "  [1]\n",
      "  [2]\n",
      "  [1]]\n",
      "\n",
      " [[1]\n",
      "  [1]\n",
      "  [0]\n",
      "  [2]]]\n",
      "\n",
      "Target sequence:\n",
      "[[1]\n",
      " [0]\n",
      " [1]]\n",
      "\n",
      "Predicted sequence:\n",
      "[[[0.99988821]]\n",
      "\n",
      " [[0.9941145 ]]\n",
      "\n",
      " [[1.00924171]]]\n"
     ]
    }
   ],
   "source": [
    "def forward(inputs, h_prev, C_prev, p):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    h_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    C_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    p -- python list containing:\n",
    "                        W_f -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_f -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                        W_i -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_i -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "                        W_g -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_g --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                        W_o -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_o --  Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "                        W_v -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_v, n_a)\n",
    "                        b_v -- Bias relating the hidden-state to the output, numpy array of shape (n_v, 1)\n",
    "    Returns:\n",
    "    z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s -- lists of size m containing the computations in each forward pass\n",
    "    outputs -- prediction at timestep \"t\", numpy array of shape (n_v, m)\n",
    "    \"\"\"\n",
    "    assert h_prev.shape == (hd, 1)\n",
    "    assert C_prev.shape == (hd, 1)\n",
    "\n",
    "    # First we unpack our parameters\n",
    "    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p\n",
    "    \n",
    "    # Save a list of computations for each of the components in the LSTM\n",
    "    x_s, z_s, f_s, i_s,  = [], [] ,[], []\n",
    "    g_s, C_s, o_s, h_s = [], [] ,[], []\n",
    "    v_s, output_s =  [], [] \n",
    "    \n",
    "    # Append the initial cell and hidden state to their respective lists\n",
    "    h_s.append(h_prev)\n",
    "    C_s.append(C_prev)\n",
    "    \n",
    "    for x in inputs:\n",
    "        # Concatenate input and hidden state\n",
    "        z = np.row_stack((h_prev, x))\n",
    "        z_s.append(z)\n",
    "        \n",
    "        # Calculate forget gate\n",
    "        f = sigmoid(np.dot(W_f, z) + b_f)\n",
    "        f_s.append(f)\n",
    "        \n",
    "        # Calculate input gate\n",
    "        i = sigmoid(np.dot(W_i, z) + b_i)\n",
    "        i_s.append(i)\n",
    "        \n",
    "        # Calculate candidate\n",
    "        g = tanh(np.dot(W_g, z) + b_g)\n",
    "        g_s.append(g)\n",
    "        \n",
    "        # Calculate memory state\n",
    "        C_prev = C_prev * f + g * i\n",
    "        C_s.append(C_prev)\n",
    "        \n",
    "        # Calculate output gate\n",
    "        o = sigmoid(np.dot(W_o, z) + b_o)\n",
    "        o_s.append(o)\n",
    "        \n",
    "        # Calculate hidden state\n",
    "        h_prev = o * tanh(C_prev)\n",
    "        h_s.append(h_prev)\n",
    "\n",
    "        # Calculate logits\n",
    "        v = np.dot(W_v, h_prev) + b_v\n",
    "        v_s.append(v)\n",
    "\n",
    "        # Calculate softmax\n",
    "        #output = softmax(v)\n",
    "        output_s.append(v)\n",
    "\n",
    "    return z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, output_s\n",
    "\n",
    "\n",
    "# Get first sentence in test set\n",
    "inputs, targets = test_set[1]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "h = np.zeros((hd, 1))\n",
    "c = np.zeros((hd, 1))\n",
    "\n",
    "# Forward pass\n",
    "z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(ipt[0], h, c, params)\n",
    "\n",
    "#output_sentence = [idx_to_word[np.argmax(output)] for output in outputs]\n",
    "print('Input sentence:')\n",
    "print(ipt[0])\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(opt[0])\n",
    "\n",
    "print('\\nPredicted sequence:')\n",
    "print(np.array(v_s))\n",
    "# print([idx_to_word[np.argmax(output)] for output in outputs])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "98254cac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T08:13:40.943338Z",
     "start_time": "2021-11-23T08:13:40.922492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.99959403]]), array([[0.99930368]]), array([[1.0007546]])]\n",
      "[[1]\n",
      " [0]\n",
      " [1]]\n",
      "We get a loss of:\n",
      "0.9986085745539479\n"
     ]
    }
   ],
   "source": [
    "def backward(z, f, i, g, C, o, h, v, outputs, targets, p = params):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    z -- your concatenated input data  as a list of size m.\n",
    "    f -- your forget gate computations as a list of size m.\n",
    "    i -- your input gate computations as a list of size m.\n",
    "    g -- your candidate computations as a list of size m.\n",
    "    C -- your Cell states as a list of size m+1.\n",
    "    o -- your output gate computations as a list of size m.\n",
    "    h -- your Hidden state computations as a list of size m+1.\n",
    "    v -- your logit computations as a list of size m.\n",
    "    outputs -- your outputs as a list of size m.\n",
    "    targets -- your targets as a list of size m.\n",
    "    p -- python list containing:\n",
    "                        W_f -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_f -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                        W_i -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_i -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "                        W_g -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_g --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                        W_o -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_o --  Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "                        W_v -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_v, n_a)\n",
    "                        b_v -- Bias relating the hidden-state to the output, numpy array of shape (n_v, 1)\n",
    "    Returns:\n",
    "    loss -- crossentropy loss for all elements in output\n",
    "    grads -- lists of gradients of every element in p\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack parameters\n",
    "    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p\n",
    "\n",
    "    # Initialize gradients as zero\n",
    "    W_f_d = np.zeros_like(W_f)\n",
    "    b_f_d = np.zeros_like(b_f)\n",
    "\n",
    "    W_i_d = np.zeros_like(W_i)\n",
    "    b_i_d = np.zeros_like(b_i)\n",
    "\n",
    "    W_g_d = np.zeros_like(W_g)\n",
    "    b_g_d = np.zeros_like(b_g)\n",
    "\n",
    "    W_o_d = np.zeros_like(W_o)\n",
    "    b_o_d = np.zeros_like(b_o)\n",
    "\n",
    "    W_v_d = np.zeros_like(W_v)\n",
    "    b_v_d = np.zeros_like(b_v)\n",
    "    \n",
    "    # Set the next cell and hidden state equal to zero\n",
    "    dh_next = np.zeros_like(h[0])\n",
    "    dC_next = np.zeros_like(C[0])\n",
    "        \n",
    "    # Track loss\n",
    "    loss = 0\n",
    "    \n",
    "    for t in reversed(range(len(outputs))):\n",
    "        # Compute the cross entropy\n",
    "#         print(outputs[t])\n",
    "#         print(targets[t])\n",
    "#         print(\"--\")\n",
    "        loss += -np.mean(np.log(outputs[t]) * targets[t])\n",
    "\n",
    "        # Get the previous hidden cell state\n",
    "        C_prev= C[t-1]\n",
    "\n",
    "        # Compute the derivative of the relation of the hidden-state to the output gate\n",
    "        dv = np.copy(outputs[t])\n",
    "        dv[np.argmax(targets[t])] -= 1\n",
    "\n",
    "        # Update the gradient of the relation of the hidden-state to the output gate\n",
    "        W_v_d += np.dot(dv, h[t].T)\n",
    "        b_v_d += dv\n",
    "\n",
    "        # Compute the derivative of the hidden state and output gate\n",
    "        dh = np.dot(W_v.T, dv)\n",
    "        dh += dh_next\n",
    "        do = dh * tanh(C[t])\n",
    "        do = sigmoid(o[t], derivative=True)*do\n",
    "\n",
    "        # Update the gradients with respect to the output gate\n",
    "        W_o_d += np.dot(do, z[t].T)\n",
    "        b_o_d += do\n",
    "\n",
    "        # Compute the derivative of the cell state and candidate g\n",
    "        dC = np.copy(dC_next)\n",
    "        dC += dh * o[t] * tanh(tanh(C[t]), derivative=True)\n",
    "        dg = dC * i[t]\n",
    "        dg = tanh(g[t], derivative=True) * dg\n",
    "\n",
    "        # Update the gradients with respect to the candidate\n",
    "        W_g_d += np.dot(dg, z[t].T)\n",
    "        b_g_d += dg\n",
    "\n",
    "        # Compute the derivative of the input gate and update its gradients\n",
    "        di = dC * g[t]\n",
    "        di = sigmoid(i[t], True) * di\n",
    "        W_i_d += np.dot(di, z[t].T)\n",
    "        b_i_d += di\n",
    "\n",
    "        # Compute the derivative of the forget gate and update its gradients\n",
    "        df = dC * C_prev\n",
    "        df = sigmoid(f[t]) * df\n",
    "        W_f_d += np.dot(df, z[t].T)\n",
    "        b_f_d += df\n",
    "\n",
    "        # Compute the derivative of the input and update the gradients of the previous hidden and cell state\n",
    "        dz = (np.dot(W_f.T, df)\n",
    "             + np.dot(W_i.T, di)\n",
    "             + np.dot(W_g.T, dg)\n",
    "             + np.dot(W_o.T, do))\n",
    "        dh_prev = dz[:hd, :]\n",
    "        dC_prev = f[t] * dC\n",
    "        \n",
    "    grads= W_f_d, W_i_d, W_g_d, W_o_d, W_v_d, b_f_d, b_i_d, b_g_d, b_o_d, b_v_d\n",
    "    \n",
    "    # Clip gradients\n",
    "    grads = clip_gradient_norm(grads)\n",
    "    \n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "# Perform a backward pass\n",
    "loss, grads = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, opt[0], params)\n",
    "print(outputs)\n",
    "print(opt[0])\n",
    "print('We get a loss of:')\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "6d58a6b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T08:14:12.650439Z",
     "start_time": "2021-11-23T08:13:46.818039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 0.03255375440705268, validation loss: 0.30621762471663067\n",
      "Epoch 10, training loss: 0.030098579940169234, validation loss: 0.24657119355941393\n",
      "Epoch 20, training loss: 0.03000270645356781, validation loss: 0.24623093618766503\n",
      "Epoch 30, training loss: 0.029943787314152752, validation loss: 0.24587515462403953\n",
      "Epoch 40, training loss: 0.02984642935891518, validation loss: 0.24503440808296978\n",
      "Epoch 50, training loss: 0.02973027949832735, validation loss: 0.24396637116246275\n",
      "Epoch 60, training loss: 0.029616932739359773, validation loss: 0.2429012223647323\n",
      "Epoch 70, training loss: 0.02951773895805065, validation loss: 0.24195725334622784\n",
      "Epoch 80, training loss: 0.029436422776626236, validation loss: 0.24117492030290966\n",
      "Epoch 90, training loss: 0.029372633596059156, validation loss: 0.2405536862321812\n",
      "Epoch 100, training loss: 0.029324260504122054, validation loss: 0.24007520454809242\n",
      "Epoch 110, training loss: 0.029288677264098457, validation loss: 0.2397156496774925\n",
      "Epoch 120, training loss: 0.029263334726879534, validation loss: 0.23945158034335384\n",
      "Epoch 130, training loss: 0.029246001389444738, validation loss: 0.239262347892726\n",
      "Epoch 140, training loss: 0.029234828315568583, validation loss: 0.23913078460202197\n",
      "Epoch 150, training loss: 0.029228333503617945, validation loss: 0.2390431005662382\n",
      "Epoch 160, training loss: 0.029225354485924948, validation loss: 0.23898846397495682\n",
      "Epoch 170, training loss: 0.02922499296549376, validation loss: 0.23895849654152954\n",
      "Epoch 180, training loss: 0.02922656226140068, validation loss: 0.23894678950123552\n",
      "Epoch 190, training loss: 0.029229541738786037, validation loss: 0.2389484817231592\n",
      "Epoch 200, training loss: 0.029233539200410504, validation loss: 0.23895991044215417\n",
      "Epoch 210, training loss: 0.02923826077613031, validation loss: 0.23897833104789548\n",
      "Epoch 220, training loss: 0.02924348729409701, validation loss: 0.23900169687270817\n",
      "Epoch 230, training loss: 0.029249055995746474, validation loss: 0.239028488596207\n",
      "Epoch 240, training loss: 0.029254846528799398, validation loss: 0.23905758345277447\n",
      "Epoch 250, training loss: 0.02926077029858612, validation loss: 0.23908815573148318\n",
      "Epoch 260, training loss: 0.029266762419668454, validation loss: 0.23911960153272427\n",
      "Epoch 270, training loss: 0.029272775660642104, validation loss: 0.2391514821348937\n",
      "Epoch 280, training loss: 0.029278775905143967, validation loss: 0.23918348152825047\n",
      "Epoch 290, training loss: 0.029284738759400086, validation loss: 0.2392153746686661\n",
      "Epoch 300, training loss: 0.02929064702267727, validation loss: 0.23924700380378813\n",
      "Epoch 310, training loss: 0.029296488804666135, validation loss: 0.2392782608541758\n",
      "Epoch 320, training loss: 0.02930225612633112, validation loss: 0.23930907432143633\n",
      "Epoch 330, training loss: 0.0293079438811015, validation loss: 0.23933939957177763\n",
      "Epoch 340, training loss: 0.02931354906403218, validation loss: 0.23936921163056865\n",
      "Epoch 350, training loss: 0.029319070199867077, validation loss: 0.2393984998412454\n",
      "Epoch 360, training loss: 0.02932450691851097, validation loss: 0.23942726390619362\n",
      "Epoch 370, training loss: 0.029329859639612207, validation loss: 0.2394555109506739\n",
      "Epoch 380, training loss: 0.029335129337834703, validation loss: 0.239483253343273\n",
      "Epoch 390, training loss: 0.029340317367766138, validation loss: 0.23951050707535143\n",
      "Epoch 400, training loss: 0.029345425332893492, validation loss: 0.23953729055331593\n",
      "Epoch 410, training loss: 0.029350454987148403, validation loss: 0.23956362369569473\n",
      "Epoch 420, training loss: 0.02935540816054256, validation loss: 0.23958952725527383\n",
      "Epoch 430, training loss: 0.02936028670264545, validation loss: 0.23961502230748497\n",
      "Epoch 440, training loss: 0.029365092439305778, validation loss: 0.2396401298616998\n",
      "Epoch 450, training loss: 0.029369827139234948, validation loss: 0.23966487056350833\n",
      "Epoch 460, training loss: 0.029374492487967994, validation loss: 0.23968926446448002\n",
      "Epoch 470, training loss: 0.029379090067379206, validation loss: 0.23971333084212013\n",
      "Epoch 480, training loss: 0.02938362133941635, validation loss: 0.23973708805731458\n",
      "Epoch 490, training loss: 0.02938808763307776, validation loss: 0.2397605534399396\n",
      "Epoch 500, training loss: 0.029392490133921256, validation loss: 0.23978374319581042\n",
      "Epoch 510, training loss: 0.029396829875590346, validation loss: 0.23980667232999653\n",
      "Epoch 520, training loss: 0.029401107732988676, validation loss: 0.2398293545829041\n",
      "Epoch 530, training loss: 0.029405324416841128, validation loss: 0.23985180237655399\n",
      "Epoch 540, training loss: 0.02940948046946123, validation loss: 0.23987402676924843\n",
      "Epoch 550, training loss: 0.029413576261603628, validation loss: 0.23989603741740087\n",
      "Epoch 560, training loss: 0.02941761199032602, validation loss: 0.2399178425437374\n",
      "Epoch 570, training loss: 0.029421587677817822, validation loss: 0.2399394489114131\n",
      "Epoch 580, training loss: 0.02942550317117782, validation loss: 0.23996086180383622\n",
      "Epoch 590, training loss: 0.029429358143141196, validation loss: 0.23998208501018664\n",
      "Epoch 600, training loss: 0.029433152093768924, validation loss: 0.24000312081675956\n",
      "Epoch 610, training loss: 0.029436884353121022, validation loss: 0.24002397000436995\n",
      "Epoch 620, training loss: 0.029440554084940306, validation loss: 0.24004463185212865\n",
      "Epoch 630, training loss: 0.029444160291374852, validation loss: 0.2400651041479512\n",
      "Epoch 640, training loss: 0.029447701818766794, validation loss: 0.24008538320618128\n",
      "Epoch 650, training loss: 0.029451177364531914, validation loss: 0.240105463892721\n",
      "Epoch 660, training loss: 0.029454585485149107, validation loss: 0.2401253396580374\n",
      "Epoch 670, training loss: 0.029457924605271572, validation loss: 0.24014500257839028\n",
      "Epoch 680, training loss: 0.029461193027962383, validation loss: 0.24016444340557122\n",
      "Epoch 690, training loss: 0.029464388946046683, validation loss: 0.24018365162538188\n",
      "Epoch 700, training loss: 0.02946751045456013, validation loss: 0.2402026155249994\n",
      "Epoch 710, training loss: 0.029470555564260464, validation loss: 0.24022132226928758\n",
      "Epoch 720, training loss: 0.029473522216154145, validation loss: 0.24023975798600433\n",
      "Epoch 730, training loss: 0.02947640829697525, validation loss: 0.24025790785974788\n",
      "Epoch 740, training loss: 0.029479211655538115, validation loss: 0.24027575623435937\n",
      "Epoch 750, training loss: 0.029481930119869364, validation loss: 0.240293286723373\n",
      "Epoch 760, training loss: 0.029484561515009656, validation loss: 0.24031048232797642\n",
      "Epoch 770, training loss: 0.029487103681359983, validation loss: 0.24032732556180936\n",
      "Epoch 780, training loss: 0.02948955449343346, validation loss: 0.24034379858180194\n",
      "Epoch 790, training loss: 0.029491911878860317, validation loss: 0.24035988332412853\n",
      "Epoch 800, training loss: 0.029494173837482526, validation loss: 0.240375561644234\n",
      "Epoch 810, training loss: 0.029496338460364653, validation loss: 0.24039081545978744\n",
      "Epoch 820, training loss: 0.02949840394854065, validation loss: 0.24040562689532094\n",
      "Epoch 830, training loss: 0.029500368631310978, validation loss: 0.24041997842723742\n",
      "Epoch 840, training loss: 0.029502230983902845, validation loss: 0.24043385302781217\n",
      "Epoch 850, training loss: 0.029503989644307015, validation loss: 0.24044723430677606\n",
      "Epoch 860, training loss: 0.02950564342910842, validation loss: 0.24046010664905323\n",
      "Epoch 870, training loss: 0.0295071913481354, validation loss: 0.24047245534723494\n",
      "Epoch 880, training loss: 0.029508632617761894, validation loss: 0.24048426672740297\n",
      "Epoch 890, training loss: 0.029509966672710632, validation loss: 0.2404955282669753\n",
      "Epoch 900, training loss: 0.029511193176221136, validation loss: 0.24050622870332566\n",
      "Epoch 910, training loss: 0.029512312028465087, validation loss: 0.2405163581320334\n",
      "Epoch 920, training loss: 0.029513323373112372, validation loss: 0.2405259080937434\n",
      "Epoch 930, training loss: 0.029514227601974183, validation loss: 0.24053487164876045\n",
      "Epoch 940, training loss: 0.029515025357673478, validation loss: 0.2405432434386599\n",
      "Epoch 950, training loss: 0.029515717534318742, validation loss: 0.2405510197343683\n",
      "Epoch 960, training loss: 0.02951630527618245, validation loss: 0.24055819847034815\n",
      "Epoch 970, training loss: 0.02951678997441132, validation loss: 0.2405647792647072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 980, training loss: 0.029517173261820837, validation loss: 0.24057076342523692\n",
      "Epoch 990, training loss: 0.02951745700585013, validation loss: 0.24057615394157414\n",
      "Epoch 1000, training loss: 0.029517643299776228, validation loss: 0.2405809554638506\n",
      "Epoch 1010, training loss: 0.029517734452306783, validation loss: 0.2405851742683704\n",
      "Epoch 1020, training loss: 0.029517732975688854, validation loss: 0.24058881821100456\n",
      "Epoch 1030, training loss: 0.02951764157248652, validation loss: 0.24059189666913344\n",
      "Epoch 1040, training loss: 0.029517463121193004, validation loss: 0.24059442047308757\n",
      "Epoch 1050, training loss: 0.02951720066085186, validation loss: 0.24059640182813768\n",
      "Epoch 1060, training loss: 0.029516857374869133, validation loss: 0.2405978542281643\n",
      "Epoch 1070, training loss: 0.029516436574200666, validation loss: 0.2405987923621951\n",
      "Epoch 1080, training loss: 0.029515941680099484, validation loss: 0.2405992320150339\n",
      "Epoch 1090, training loss: 0.029515376206605486, validation loss: 0.24059918996321766\n",
      "Epoch 1100, training loss: 0.029514743742953868, validation loss: 0.24059868386753572\n",
      "Epoch 1110, training loss: 0.029514047936071475, validation loss: 0.24059773216331684\n",
      "Epoch 1120, training loss: 0.02951329247331969, validation loss: 0.24059635394964957\n",
      "Epoch 1130, training loss: 0.02951248106563113, validation loss: 0.2405945688786424\n",
      "Epoch 1140, training loss: 0.029511617431174066, validation loss: 0.24059239704576463\n",
      "Epoch 1150, training loss: 0.029510705279663915, validation loss: 0.2405898588822193\n",
      "Epoch 1160, training loss: 0.02950974829742659, validation loss: 0.24058697505022092\n",
      "Epoch 1170, training loss: 0.029508750133302182, validation loss: 0.24058376634194764\n",
      "Epoch 1180, training loss: 0.02950771438546263, validation loss: 0.24058025358284255\n",
      "Epoch 1190, training loss: 0.02950664458920106, validation loss: 0.24057645753983967\n",
      "Epoch 1200, training loss: 0.029505544205735524, validation loss: 0.24057239883498674\n",
      "Epoch 1210, training loss: 0.02950441661205577, validation loss: 0.24056809786484387\n",
      "Epoch 1220, training loss: 0.029503265091827897, validation loss: 0.2405635747259395\n",
      "Epoch 1230, training loss: 0.029502092827359386, validation loss: 0.24055884914647915\n",
      "Epoch 1240, training loss: 0.029500902892615536, validation loss: 0.24055394042441589\n",
      "Epoch 1250, training loss: 0.029499698247268086, validation loss: 0.24054886737191772\n",
      "Epoch 1260, training loss: 0.02949848173174793, validation loss: 0.24054364826619715\n",
      "Epoch 1270, training loss: 0.02949725606326582, validation loss: 0.24053830080660313\n",
      "Epoch 1280, training loss: 0.029496023832758678, validation loss: 0.2405328420778255\n",
      "Epoch 1290, training loss: 0.02949478750271365, validation loss: 0.24052728851901292\n",
      "Epoch 1300, training loss: 0.02949354940581769, validation loss: 0.24052165589856453\n",
      "Epoch 1310, training loss: 0.029492311744377716, validation loss: 0.2405159592943259\n",
      "Epoch 1320, training loss: 0.029491076590453746, validation loss: 0.2405102130788924\n",
      "Epoch 1330, training loss: 0.02948984588664652, validation loss: 0.24050443090970436\n",
      "Epoch 1340, training loss: 0.029488621447480542, validation loss: 0.2404986257236039\n",
      "Epoch 1350, training loss: 0.029487404961323783, validation loss: 0.2404928097355168\n",
      "Epoch 1360, training loss: 0.0294861979927863, validation loss: 0.2404869944409173\n",
      "Epoch 1370, training loss: 0.029485001985541208, validation loss: 0.2404811906217352\n",
      "Epoch 1380, training loss: 0.0294838182655138, validation loss: 0.24047540835536785\n",
      "Epoch 1390, training loss: 0.029482648044386384, validation loss: 0.2404696570264701\n",
      "Epoch 1400, training loss: 0.02948149242336945, validation loss: 0.2404639453411987\n",
      "Epoch 1410, training loss: 0.029480352397192135, validation loss: 0.2404582813436076\n",
      "Epoch 1420, training loss: 0.02947922885826834, validation loss: 0.24045267243389876\n",
      "Epoch 1430, training loss: 0.02947812260099742, validation loss: 0.2404471253882485\n",
      "Epoch 1440, training loss: 0.029477034326161937, validation loss: 0.24044164637994966\n",
      "Epoch 1450, training loss: 0.029475964645387792, validation loss: 0.24043624100162184\n",
      "Epoch 1460, training loss: 0.029474914085635174, validation loss: 0.2404309142882619\n",
      "Epoch 1470, training loss: 0.02947388309369179, validation loss: 0.240425670740922\n",
      "Epoch 1480, training loss: 0.02947287204064284, validation loss: 0.24042051435082284\n",
      "Epoch 1490, training loss: 0.029471881226294936, validation loss: 0.24041544862372347\n",
      "Epoch 1500, training loss: 0.02947091088353379, validation loss: 0.24041047660438708\n",
      "Epoch 1510, training loss: 0.02946996118259829, validation loss: 0.24040560090099788\n",
      "Epoch 1520, training loss: 0.029469032235255564, validation loss: 0.24040082370939994\n",
      "Epoch 1530, training loss: 0.02946812409886454, validation loss: 0.24039614683704355\n",
      "Epoch 1540, training loss: 0.02946723678031671, validation loss: 0.24039157172653677\n",
      "Epoch 1550, training loss: 0.02946637023984569, validation loss: 0.24038709947871684\n",
      "Epoch 1560, training loss: 0.02946552439469821, validation loss: 0.24038273087516374\n",
      "Epoch 1570, training loss: 0.02946469912266117, validation loss: 0.2403784664000934\n",
      "Epoch 1580, training loss: 0.02946389426544091, validation loss: 0.24037430626157755\n",
      "Epoch 1590, training loss: 0.029463109631891776, validation loss: 0.24037025041204543\n",
      "Epoch 1600, training loss: 0.02946234500109292, validation loss: 0.2403662985680344\n",
      "Epoch 1610, training loss: 0.02946160012527263, validation loss: 0.24036245022916244\n",
      "Epoch 1620, training loss: 0.02946087473258104, validation loss: 0.24035870469630422\n",
      "Epoch 1630, training loss: 0.029460168529712515, validation loss: 0.24035506108895877\n",
      "Epoch 1640, training loss: 0.02945948120437984, validation loss: 0.2403515183618028\n",
      "Epoch 1650, training loss: 0.0294588124276432, validation loss: 0.2403480753204295\n",
      "Epoch 1660, training loss: 0.02945816185609685, validation loss: 0.24034473063627626\n",
      "Epoch 1670, training loss: 0.029457529133917682, validation loss: 0.2403414828607503\n",
      "Epoch 1680, training loss: 0.029456913894779644, validation loss: 0.24033833043856567\n",
      "Epoch 1690, training loss: 0.02945631576363822, validation loss: 0.24033527172030417\n",
      "Epoch 1700, training loss: 0.029455734358390086, validation loss: 0.2403323049742204\n",
      "Epoch 1710, training loss: 0.029455169291412345, validation loss: 0.24032942839730992\n",
      "Epoch 1720, training loss: 0.029454620170986642, validation loss: 0.24032664012566407\n",
      "Epoch 1730, training loss: 0.029454086602613037, validation loss: 0.24032393824413276\n",
      "Epoch 1740, training loss: 0.029453568190218694, validation loss: 0.24032132079532215\n",
      "Epoch 1750, training loss: 0.029453064537266718, validation loss: 0.24031878578795207\n",
      "Epoch 1760, training loss: 0.029452575247769848, validation loss: 0.240316331204599\n",
      "Epoch 1770, training loss: 0.02945209992721435, validation loss: 0.24031395500885233\n",
      "Epoch 1780, training loss: 0.029451638183398778, validation loss: 0.24031165515190941\n",
      "Epoch 1790, training loss: 0.029451189627192665, validation loss: 0.24030942957863793\n",
      "Epoch 1800, training loss: 0.029450753873219552, validation loss: 0.240307276233131\n",
      "Epoch 1810, training loss: 0.02945033054046929, validation loss: 0.24030519306378145\n",
      "Epoch 1820, training loss: 0.029449919252843737, validation loss: 0.2403031780279028\n",
      "Epoch 1830, training loss: 0.029449519639640338, validation loss: 0.24030122909591953\n",
      "Epoch 1840, training loss: 0.029449131335977586, validation loss: 0.24029934425515506\n",
      "Epoch 1850, training loss: 0.029448753983166407, validation loss: 0.240297521513238\n",
      "Epoch 1860, training loss: 0.029448387229031252, validation loss: 0.24029575890115293\n",
      "Epoch 1870, training loss: 0.029448030728184464, validation loss: 0.24029405447595717\n",
      "Epoch 1880, training loss: 0.02944768414225743, validation loss: 0.2402924063231845\n",
      "Epoch 1890, training loss: 0.029447347140091833, validation loss: 0.24029081255895846\n",
      "Epoch 1900, training loss: 0.029447019397894063, validation loss: 0.24028927133183442\n",
      "Epoch 1910, training loss: 0.02944670059935574, validation loss: 0.2402877808243891\n",
      "Epoch 1920, training loss: 0.029446390435743257, validation loss: 0.24028633925457715\n",
      "Epoch 1930, training loss: 0.029446088605958848, validation loss: 0.24028494487687133\n",
      "Epoch 1940, training loss: 0.029445794816575742, validation loss: 0.24028359598320248\n",
      "Epoch 1950, training loss: 0.029445508781849716, validation loss: 0.2402822909037163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1960, training loss: 0.02944523022370931, validation loss: 0.24028102800736106\n",
      "Epoch 1970, training loss: 0.02944495887172661, validation loss: 0.24027980570231938\n",
      "Epoch 1980, training loss: 0.029444694463070697, validation loss: 0.24027862243629905\n",
      "Epoch 1990, training loss: 0.029444436742445425, validation loss: 0.24027747669669444\n"
     ]
    }
   ],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve inputs and targets at the given index\n",
    "        X = self.inputs[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "validSet=Dataset(ipt, opt)\n",
    "\n",
    "trainSet=Dataset(ipt, opt)\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 2000\n",
    "\n",
    "# Initialize a new network\n",
    "hd=50\n",
    "z_size = hd + numFeats # Size of concatenated hidden + input vector\n",
    "params = init_lstm(hd, numFeats, z=z_size)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hd, 1))\n",
    "\n",
    "# Track loss\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "# For each epoch\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "\n",
    "    # For each sentence in validation set\n",
    "    for inputs, targets in validSet:\n",
    "        \n",
    "        # Initialize hidden state and cell state as zeros\n",
    "        h = np.zeros((hd, 1))\n",
    "        c = np.zeros((hd, 1))\n",
    "\n",
    "        # Forward pass\n",
    "        z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs, h, c, params)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss, _ = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets, params)\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_validation_loss += loss\n",
    "    \n",
    "    # For each sentence in training set\n",
    "    for inputs, targets in trainSet:\n",
    "        \n",
    "        # Initialize hidden state and cell state as zeros\n",
    "        h = np.zeros((hd, 1))\n",
    "        c = np.zeros((hd, 1))\n",
    "\n",
    "        # Forward pass\n",
    "        z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs, h, c, params)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss, grads = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets, params)\n",
    "        \n",
    "        # Update parameters\n",
    "        params = update_parameters(params, grads, lr=1e-1)\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_training_loss += loss\n",
    "                \n",
    "    # Save loss for plot\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "219abc7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-23T08:14:16.928168Z",
     "start_time": "2021-11-23T08:14:16.920860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1.00077163]]), array([[0.99965776]]), array([[1.00157559]])]\n",
      "[[0]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(ipt[1], h, c, params)\n",
    "print(v_s)\n",
    "print(opt[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35e96ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
