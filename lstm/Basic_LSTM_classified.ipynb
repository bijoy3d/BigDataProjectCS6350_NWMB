{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef77dc2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T00:45:58.591503Z",
     "start_time": "2021-11-26T00:45:58.091034Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a7b285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T00:45:58.602888Z",
     "start_time": "2021-11-26T00:45:58.593696Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset=StringIO(\"\"\"Date,Open,High,Low,Close,Volume,Trade_count,Vwap\n",
    "2015-12-01 09:00:00+00:00,118.88,118.94,118.88,118.94,1145,5,118.902052\n",
    "2015-12-01 09:15:00+00:00,118.77,118.77,118.77,118.77,200,1,118.77\n",
    "2015-12-01 09:30:00+00:00,118.69,118.69,118.6,118.6,900,4,118.61\n",
    "2015-12-01 09:45:00+00:00,118.64,118.65,118.64,118.65,3580,5,118.648883\n",
    "2015-12-01 10:00:00+00:00,118.65,118.65,118.55,118.55,1820,4,118.611538\n",
    "2015-12-01 10:15:00+00:00,118.55,118.6,118.55,118.6,880,5,118.5625\n",
    "2015-12-01 10:30:00+00:00,118.55,118.55,118.5,118.5,1878,5,118.513312\n",
    "2015-12-01 10:45:00+00:00,118.59,118.72,118.59,118.72,2499,10,118.628431\n",
    "2015-12-01 11:00:00+00:00,118.71,118.9,118.71,118.9,2842,11,118.86064\n",
    "2015-12-01 11:15:00+00:00,118.87,118.87,118.87,118.87,300,2,118.87\n",
    "2015-12-01 11:30:00+00:00,118.78,118.8,118.76,118.8,3914,22,118.785876\n",
    "2015-12-01 11:45:00+00:00,118.8,118.99,118.77,118.9,7900,37,118.893542\n",
    "2015-12-01 12:00:00+00:00,118.88,118.98,118.84,118.84,6540,34,118.922648\n",
    "2015-12-01 12:15:00+00:00,118.82,118.84,118.77,118.77,5603,28,118.804962\n",
    "2015-12-01 12:30:00+00:00,118.77,118.89,118.76,118.88,7612,31,118.824002\n",
    "\"\"\")\n",
    "df = pd.read_table(dataset, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44a26e82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T00:45:58.613002Z",
     "start_time": "2021-11-26T00:45:58.604917Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Computes the element-wise sigmoid activation function for an array x.\n",
    "\n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    return 1 / (1 + np.exp(-x_safe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7870a92d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T00:45:58.713464Z",
     "start_time": "2021-11-26T00:45:58.615230Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM():\n",
    "    def __init__(self, train_data, targets, batch_size=2, debug=1, test=1):\n",
    "        #4 gates\n",
    "        # input_activation = tanh(wa [inner] input + ua [inner] prev_output + ba)\n",
    "        # input_gate  = sigmoid(wi [inner] input + ui [inner] prev_output + bi)\n",
    "        # forget_gate = sigmoid(wf [inner] input + uf [inner] prev_output + bf)\n",
    "        # output_gate = sigmoid(wo [inner] input + uo [inner] prev_output + bo)\n",
    "\n",
    "        # 2 states\n",
    "        # internal_state = (input_activation [Element wise] input_gate) + (forget_gate [Element wise] prev_internal_state)\n",
    "        # output = tanh(internal_state) [Element wise] output_gate\n",
    "\n",
    "        if batch_size >= len(train_data):\n",
    "            print(f'Batch Size {batch_size} should be less than the size of the dataset {len(train_data)}')\n",
    "            return None\n",
    "        \n",
    "        # To enable test mode. Batch size would be set as 2\n",
    "        self.test = test\n",
    "        \n",
    "        # The number of records that would go inside the LSTM at one time. A sequence of records.\n",
    "        self.batch_size = batch_size\n",
    "        numFeats = train_data.shape[1] ###### CHANGE IT TO GET DYNAMICALLY FROM INPUT\n",
    "        # Enable debug logs\n",
    "        self.debug = debug\n",
    "        \n",
    "        # Training Data\n",
    "        self.train_data = train_data\n",
    "        # Target of training data\n",
    "        self.targets = targets\n",
    "        \n",
    "        \n",
    "        # input_activation\n",
    "        self.wa = np.random.random((numFeats, 1))\n",
    "        self.ua = np.random.random((1, 1))\n",
    "        self.ba = np.random.random((1, 1))\n",
    "\n",
    "        # input_gate\n",
    "        self.wi = np.random.random((numFeats, 1))\n",
    "        self.ui = np.random.random((1, 1))\n",
    "        self.bi = np.random.random((1, 1))\n",
    "\n",
    "        # forget_gate\n",
    "        self.wf = np.random.random((numFeats, 1))\n",
    "        self.uf = np.random.random((1, 1))\n",
    "        self.bf = np.random.random((1, 1))\n",
    "\n",
    "        # output_gate\n",
    "        self.wo = np.random.random((numFeats, 1))\n",
    "        self.uo = np.random.random((1, 1))\n",
    "        self.bo = np.random.random((1, 1))\n",
    "\n",
    "        # Clean and init LSTM\n",
    "        self.cleanLSTM()\n",
    "\n",
    "\n",
    "        if self.test:\n",
    "            self.batchSize = 1\n",
    "            self.wa[0]=0.45\n",
    "            self.wa[1]=0.25\n",
    "            self.ua[0]=0.15\n",
    "            self.ba[0]=0.2\n",
    "            self.wi[0]=0.95\n",
    "            self.wi[1]=0.8\n",
    "            self.ui[0]=0.8\n",
    "            self.bi[0]=0.65\n",
    "            self.wf[0]=0.7\n",
    "            self.wf[1]=0.45\n",
    "            self.uf[0]=0.1\n",
    "            self.bf[0]=0.15\n",
    "            self.wo[0]=0.6\n",
    "            self.wo[1]=0.4\n",
    "            self.uo[0]=0.25\n",
    "            self.bo[0]=0.1\n",
    "        \n",
    "    def cleanLSTM(self):\n",
    "        # Forward Propogation Parameters\n",
    "        self.prev_input_activation = 0\n",
    "        self.prev_input_gate = 0\n",
    "        self.prev_forget_gate = 0\n",
    "        self.prev_output_gate = 0\n",
    "        \n",
    "        self.input_activation = 0\n",
    "        self.input_gate = 0\n",
    "        self.forget_gate = 0\n",
    "        self.output_gate = 0\n",
    "        self.internal_state = np.zeros((1, 1))\n",
    "        self.output = np.zeros((1, 1))\n",
    "\n",
    "        self.prev_input_activations = []\n",
    "        self.prev_input_gates  = []\n",
    "        self.prev_output_gates  = []\n",
    "        self.prev_forget_gates  = []\n",
    "        self.prev_internal_states  = []\n",
    "        self.prev_outputs = []\n",
    "        \n",
    "        \n",
    "        # Backward Propogation Parameters\n",
    "        self.stacked_ip_weights = []\n",
    "        self.stacked_op_weights = []\n",
    "        \n",
    "        self.der_internal_state_future = np.zeros((1, 1))\n",
    "        self.delta_op_future = np.zeros((1, 1))\n",
    "        \n",
    "        self.input_weight_derivatives = 0\n",
    "        self.output_weight_derivatives = 0\n",
    "        self.bias_derivatives = 0\n",
    "\n",
    "    def update_lstmData(self, lr=.01):\n",
    "        wa, ua, ba, wi, ui, bi, wf, uf, bf, wo, uo, bo = self.getLSTMparms()\n",
    "        dip = self.input_weight_derivatives\n",
    "        dop = self.output_weight_derivatives\n",
    "        db = self.bias_derivatives\n",
    "        \n",
    "        t = wa.T - (dip*lr)\n",
    "        wa.T[0] = t[0].T\n",
    "        t = wi.T - (dip*lr)\n",
    "        wi.T[0] = t[1].T\n",
    "        t = wf.T - (dip*lr)\n",
    "        wf.T[0] = t[2].T\n",
    "        t = wo.T - (dip*lr)\n",
    "        wo.T[0] = t[3].T\n",
    "        ua = ua.T - (dop*lr)[0]\n",
    "        ui = ui.T - (dop*lr)[1]\n",
    "        uf = uf.T - (dop*lr)[2]\n",
    "        uo = uo.T - (dop*lr)[3]\n",
    "        ba = ba.T - (db*lr)[0]\n",
    "        bi = bi.T - (db*lr)[1]\n",
    "        bf = bf.T - (db*lr)[2]\n",
    "        bo = bo.T - (db*lr)[3]\n",
    "        self.wa=wa\n",
    "        self.ua=ua\n",
    "        self.ba=ba\n",
    "        self.wi=wi\n",
    "        self.ui=ui\n",
    "        self.bi=bi\n",
    "        self.wf=wf\n",
    "        self.uf=uf\n",
    "        self.bf=bf\n",
    "        self.wo=wo\n",
    "        self.uo=uo\n",
    "        self.bo=bo\n",
    "\n",
    "    def printLSTMparms(self):\n",
    "        lstmData = self.getLSTMparms()\n",
    "        print('wa:', lstmData[0].shape)\n",
    "        print('ua:', lstmData[1].shape)\n",
    "        print('ba:', lstmData[2].shape)\n",
    "        print('wi:', lstmData[3].shape)\n",
    "        print('ui:', lstmData[4].shape)\n",
    "        print('bi:', lstmData[5].shape)\n",
    "        print('wf:', lstmData[6].shape)\n",
    "        print('uf:', lstmData[7].shape)\n",
    "        print('bf:', lstmData[8].shape)\n",
    "        print('wo:', lstmData[9].shape)\n",
    "        print('uo:', lstmData[10].shape)\n",
    "        print('bo:', lstmData[11].shape)\n",
    "\n",
    "\n",
    "        print('wa:', lstmData[0])\n",
    "        print('ua:', lstmData[1])\n",
    "        print('ba:', lstmData[2])\n",
    "        print('wi:', lstmData[3])\n",
    "        print('ui:', lstmData[4])\n",
    "        print('bi:', lstmData[5])\n",
    "        print('wf:', lstmData[6])\n",
    "        print('uf:', lstmData[7])\n",
    "        print('bf:', lstmData[8])\n",
    "        print('wo:', lstmData[9])\n",
    "        print('uo:', lstmData[10])\n",
    "        print('bo:', lstmData[11])\n",
    "\n",
    "    def lstm_data_transform(self, ip=None):\n",
    "        \"\"\" Changes data to the format for LSTM training \n",
    "    for sliding window approach \"\"\"\n",
    "        # Prepare the list for the transformed data\n",
    "        X, y = list(), list()\n",
    "        if ip is not None:\n",
    "            data = ip\n",
    "        else:\n",
    "            data = self.train_data\n",
    "        # Loop of the entire data set\n",
    "        for i in range(data.shape[0]):\n",
    "            # compute a new (sliding window) index\n",
    "            end_ix = i + self.batch_size\n",
    "\n",
    "            # if index is larger than the size of the dataset, we stop\n",
    "            if end_ix >= data.shape[0]:\n",
    "                break\n",
    "            # Get a sequence of data for x\n",
    "            seq_X = data[i:end_ix]\n",
    "            # Get only the last element of the sequency for y\n",
    "            seq_y = self.targets[i:end_ix]\n",
    "            # Append the list with sequencies\n",
    "            X.append(seq_X)\n",
    "            y.append(seq_y)\n",
    "        # Make final arrays\n",
    "        x_array = np.array(X)\n",
    "        y_array = np.array(y)\n",
    "        return x_array, y_array\n",
    "    \n",
    "    def plog(self, *msg, f=0):\n",
    "        if self.debug or f:\n",
    "            print(*msg)\n",
    "\n",
    "    def setLSTMparms(self, parms):\n",
    "        self.wa, self.ua, self.ba, self.wi, self.ui, self.bi, self.wf, self.uf, self.bf, self.wo, self.uo, self.bo = parms\n",
    "        \n",
    "    def getLSTMparms(self):\n",
    "        return self.wa, self.ua, self.ba, self.wi, self.ui, self.bi, self.wf, self.uf, self.bf, self.wo, self.uo, self.bo\n",
    "\n",
    "    def goForward(self, ipt, train=1):\n",
    "        #4 gates\n",
    "        # input_activation = tanh(wa [inner] input + ua [inner] prev_output + ba)\n",
    "        # input_gate  = sigmoid(wi [inner] input + ui [inner] prev_output + bi)\n",
    "        # forget_gate = sigmoid(wf [inner] input + uf [inner] prev_output + bf)\n",
    "        # output_gate = sigmoid(wo [inner] input + uo [inner] prev_output + bo)\n",
    "\n",
    "        # 2 states\n",
    "        # internal_state = (input_activation [Element wise] input_gate) + (forget_gate [Element wise] prev_internal_state)\n",
    "        # output = tanh(internal_state) [Element wise] output_gate\n",
    "    \n",
    "        plog = self.plog\n",
    "        wa, ua, ba, wi, ui, bi, wf, uf, bf, wo, uo, bo = self.getLSTMparms()\n",
    "        \n",
    "        po = self.output\n",
    "        ps = self.internal_state\n",
    "        plog(\"wa : \", wa.T)\n",
    "        plog(\"ipt : \", ipt)\n",
    "        plog(\"ua : \", ua)\n",
    "        plog(\"po : \", po)\n",
    "        plog(\"ba : \", ba)\n",
    "\n",
    "        plog(\"ipt T shape\", ipt.T.shape)\n",
    "        plog(\"po shape\", po.shape)\n",
    "            \n",
    "           # print(\"incoming input = \",ippo.T)\n",
    "\n",
    "        input_plus_prev_output = np.row_stack((ipt.T, po))\n",
    "        ippo = input_plus_prev_output\n",
    "\n",
    "            \n",
    "        # input activation\n",
    "        self.input_activation = np.tanh((np.inner(wa.T, ipt)) + (np.inner(ua, po)) + ba)\n",
    "        ia = self.input_activation\n",
    "\n",
    "        # input gate\n",
    "        self.input_gate = sigmoid((np.inner(wi.T, ipt)) + (np.inner(ui, po)) + bi)\n",
    "\n",
    "        # forget gate\n",
    "        self.forget_gate = sigmoid((np.inner(wf.T, ipt)) + (np.inner(uf, po)) + bf)\n",
    "        \n",
    "        # output gate\n",
    "        self.output_gate = sigmoid((np.inner(wo.T, ipt)) + (np.inner(uo, po)) + bo)\n",
    "\n",
    "        # internal state\n",
    "        self.internal_state = (np.multiply(ia, self.input_gate)) + (np.multiply(self.forget_gate, ps))\n",
    "\n",
    "        # output\n",
    "        self.output = np.multiply(np.tanh(self.internal_state), self.output_gate)\n",
    "        \n",
    "        if train:\n",
    "            self.prev_input_activations.append(ia)\n",
    "            self.prev_input_gates.append(self.input_gate)\n",
    "            self.prev_forget_gates.append(self.forget_gate)\n",
    "            self.prev_output_gates.append(self.output_gate)\n",
    "            self.prev_internal_states.append(self.internal_state)\n",
    "            self.prev_outputs.append(self.output)\n",
    "\n",
    "        plog(\"input_activation = \",ia)\n",
    "        plog(\"input gate : \", self.input_gate)\n",
    "        plog(\"forget gate : \", self.forget_gate)\n",
    "        plog(\"output gate : \",self.output_gate)\n",
    "        plog(\"internal state\", self.internal_state)\n",
    "        plog(\"output = \",self.output)\n",
    "        plog(\"----------------------------------\")\n",
    "        return self.output\n",
    "        \n",
    "    def stackWeights(self):\n",
    "        stacked_ip_weights = np.copy(self.wa)\n",
    "        stacked_ip_weights = np.column_stack((stacked_ip_weights, self.wi))\n",
    "        stacked_ip_weights = np.column_stack((stacked_ip_weights, self.wf))\n",
    "        stacked_ip_weights = np.column_stack((stacked_ip_weights, self.wo))\n",
    "        self.stacked_ip_weights = stacked_ip_weights\n",
    "        \n",
    "        stacked_op_weights = np.copy(self.ua)\n",
    "        stacked_op_weights = np.column_stack((stacked_op_weights, self.ui))\n",
    "        stacked_op_weights = np.column_stack((stacked_op_weights, self.uf))\n",
    "        stacked_op_weights = np.column_stack((stacked_op_weights, self.uo))\n",
    "        self.stacked_op_weights = stacked_op_weights\n",
    "\n",
    "    def travelBack(self, targets, inputs):\n",
    "\n",
    "        plog = self.plog\n",
    "        # Unpack parameters\n",
    "        wa, ua, ba, wi, ui, bi, wf, uf, bf, wo, uo, bo = self.getLSTMparms()\n",
    "        tempo = np.zeros((1, 1))\n",
    "        loss=0\n",
    "        plog(\"Targets is\",targets)\n",
    "        plog(\"Inputs is\",inputs)\n",
    "\n",
    "        for t in reversed(range(len(self.prev_outputs))):\n",
    "\n",
    "            output = self.prev_outputs[t]\n",
    "            target = targets[t]\n",
    "            \n",
    "            next_forget_gate = np.zeros((1, 1)) if (t==len(self.prev_outputs)-1) else self.prev_forget_gates[t+1]\n",
    "            \n",
    "            plog(\"previous outputs = \", str(self.prev_outputs))\n",
    "            plog(\"target = \",str(target))\n",
    "            plog(\"output = \", str(output))\n",
    "            \n",
    "            # Track loss\n",
    "            loss = (np.power((target - output),2))/2\n",
    "            plog(\"loss = \", str(loss), f=0)\n",
    "\n",
    "            # derivative of loss with respect to output\n",
    "            der_loss_wrt_output = output - target\n",
    "            plog(\"der_loss_wrt_output = \", der_loss_wrt_output)\n",
    "\n",
    "            # derivative of output\n",
    "            der_output = der_loss_wrt_output + self.delta_op_future\n",
    "            plog(\"der_output = \", der_output)\n",
    "\n",
    "            # derivative of internal state\n",
    "            pog = self.prev_output_gates[t]\n",
    "            ps = self.prev_internal_states[t]\n",
    "            dfis = der_output * pog * (1 - (np.tanh(ps))**2 ) + (self.der_internal_state_future * next_forget_gate)\n",
    "            self.der_internal_state_future = dfis\n",
    "            plog(\"der internal state = \", dfis)\n",
    "            plog(\"pog : \", pog)\n",
    "            plog(\"ps : \", ps)\n",
    "\n",
    "\n",
    "            pig = self.prev_input_gates[t]\n",
    "            pia = self.prev_input_activations[t]\n",
    "            der_input_activation = dfis * pig * (1 - pia**2)\n",
    "            plog(\"der_input_activation = \", der_input_activation)\n",
    "            stacked_ders = np.copy(der_input_activation)\n",
    "\n",
    "            der_inputg = dfis * pia * pig * (1 - pig)\n",
    "            stacked_ders = np.row_stack((stacked_ders, der_inputg))\n",
    "            plog(\"der_input = \", der_inputg)\n",
    "\n",
    "            pps = tempo if t==0 else self.prev_internal_states[t-1] \n",
    "            pfg = self.prev_forget_gates[t]\n",
    "            der_forgetg = dfis * pps * pfg * (1 - pfg)\n",
    "            stacked_ders = np.row_stack((stacked_ders, der_forgetg))\n",
    "            plog(\"der_forget = \", der_forgetg)   \n",
    "            \n",
    "            plog(\"pps : \", pps, t-1)\n",
    "            plog(\"pfg : \", pfg)\n",
    "            plog(\"dfis : \", str(dfis))\n",
    "\n",
    "            der_outputg = der_output * np.tanh(ps) * pog * (1 - pog)\n",
    "            stacked_ders = np.row_stack((stacked_ders, der_outputg))\n",
    "            plog(\"der_output = \", der_outputg)\n",
    "\n",
    "            self.stackWeights()\n",
    "\n",
    "\n",
    "            der_input_state = np.dot(self.stacked_ip_weights, stacked_ders)\n",
    "            plog(\"der_input_state = \", der_input_state)\n",
    "\n",
    "            der_output_state = np.dot(self.stacked_op_weights, stacked_ders)\n",
    "            plog(\"der_output_state = \", der_output_state)\n",
    "            self.delta_op_future = der_output_state\n",
    "\n",
    "            plog(\"inputs t is : \",str(t), np.array([inputs[0][t]]))\n",
    "            der_input_weight = np.dot(stacked_ders, np.array([inputs[0][t]]))\n",
    "            self.input_weight_derivatives += der_input_weight\n",
    "            plog(\"der_input_weight : \", der_input_weight)\n",
    "\n",
    "            po = tempo if t==0 else self.prev_outputs[t-1] \n",
    "            der_op_weight = np.dot(stacked_ders, po)\n",
    "            self.output_weight_derivatives += der_op_weight\n",
    "            plog(\"der_op_weight : \", der_op_weight)\n",
    "\n",
    "            self.bias_derivatives += stacked_ders\n",
    "        return loss\n",
    "    \n",
    "    def train(self, epoch=2, lr=.01):\n",
    "        plog = self.plog\n",
    "        ip_batches, op_batches = self.lstm_data_transform()\n",
    "        #print(op_batches)\n",
    "        count = 1\n",
    "        for runit in range (epoch):\n",
    "            plog(\"Running EPOCH \", runit+1, f=0)\n",
    "\n",
    "            for ipbatch,opbatch in zip(ip_batches, op_batches):\n",
    "                self.cleanLSTM()\n",
    "                plog(\"Round \"+str(count),\" ipbatch is : \", ipbatch)\n",
    "                plog(\"Round \"+str(count),\" opbatch is : \", opbatch)\n",
    "                for ip in ipbatch:\n",
    "                    plog(\"Round \"+str(count),\" ip is \",ip)\n",
    "                    self.goForward(np.array([ip]))\n",
    "                loss = self.travelBack(opbatch, np.array([ipbatch]))\n",
    "                plog(\"Round \"+str(count),\" Forward and Backward DONE\", f=0)\n",
    "                plog(\"Round \"+str(count),\" OP DONE\")\n",
    "                plog(\"Round \"+str(count),\" OLD WEIGHTS\")\n",
    "                #self.printLSTMparms()\n",
    "                self.update_lstmData(lr)\n",
    "                plog(\"Round \"+str(count), \" NEW WEIGHTS\")\n",
    "                #self.printLSTMparms()\n",
    "                count+=1\n",
    "            if runit % 100 ==0:\n",
    "                print(\"loss at epoch\",runit,\"is \", loss)\n",
    "    \n",
    "    def goPredict(self, inputs, opscaler=None, ipscaler=None):\n",
    "        plog = self.plog\n",
    "        ip_batches, _ = self.lstm_data_transform(inputs)\n",
    "        count = 1\n",
    "\n",
    "        for ipbatch in ip_batches:\n",
    "            self.cleanLSTM()\n",
    "            plog(\"Round \"+str(count),\" ipbatch is : \", ipbatch)\n",
    "\n",
    "            for ip in ipbatch:\n",
    "                plog(\"Round \"+str(count),\" ip is \",ip)\n",
    "                output = self.goForward(np.array([ip]), train=0)\n",
    "                if ipscaler and opscaler:\n",
    "                    print(f'Current Price : {round(ipscaler.inverse_transform(np.array([ip]))[0][0],3)} \\\n",
    "                            Next Price : {round(opscaler.inverse_transform(output)[0][0], 3)} \\n')\n",
    "                else:\n",
    "                    print(f'input {ip} output {output}')\n",
    "\n",
    "            count+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a224017",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T00:45:58.749568Z",
     "start_time": "2021-11-26T00:45:58.715420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 0 is  [[0.04094684]]\n"
     ]
    }
   ],
   "source": [
    "ip = np.array([[1,2],[0.5,3]])\n",
    "op= np.array([ [0.5],[1.25]])\n",
    "\n",
    "ip = np.array([[1,2],[0.5,3],[1,2],[0.5,3],[1,2],[0.5,3]])\n",
    "op= np.array([ [0.2],[0.8],[0.2],[0.8],[0.2],[0.8] ])\n",
    "\n",
    "lstm = LSTM(train_data=ip, targets=op, batch_size=2, debug=0, test=0)\n",
    "lstm.train(epoch=3, lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37991b9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T00:45:58.759593Z",
     "start_time": "2021-11-26T00:45:58.751154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input [1. 2.] output [[0.37576104]]\n",
      "input [0.5 3. ] output [[0.61743775]]\n",
      "input [0.5 3. ] output [[0.4099179]]\n",
      "input [1. 2.] output [[0.61006784]]\n",
      "input [1. 2.] output [[0.37576104]]\n",
      "input [0.5 3. ] output [[0.61743775]]\n",
      "input [0.5 3. ] output [[0.4099179]]\n",
      "input [1. 2.] output [[0.61006784]]\n"
     ]
    }
   ],
   "source": [
    "ip = np.array([[1,2],[0.5,3],[1,2],[0.5,3],[1,2],[0.5,3]])\n",
    "op= np.array([ [0.5],[1.25],[0.5],[1.25],[0.5],[1.25] ])\n",
    "\n",
    "lstm.goPredict(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70c7f389",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T00:45:59.259579Z",
     "start_time": "2021-11-26T00:45:58.761122Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "opscaler = MinMaxScaler()\n",
    "ipscaler = MinMaxScaler()\n",
    "inputs=df.copy()\n",
    "inputs.drop(\"Date\", axis=1, inplace=True)\n",
    "\n",
    "targets = inputs.filter([\"Open\"], axis=1)\n",
    "targets.columns = ['target']\n",
    "targets[\"target\"]=targets['target'][1:].reset_index(drop=True)\n",
    "\n",
    "inputs[['Open','High','Low','Close','Volume','Trade_count','Vwap']] = ipscaler.fit_transform(inputs[['Open','High','Low','Close','Volume','Trade_count','Vwap']])\n",
    "targets[['target']] = opscaler.fit_transform(targets[['target']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89de4a52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T00:48:10.215618Z",
     "start_time": "2021-11-26T00:45:59.261974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at epoch 0 is  [[0.00790581]]\n",
      "loss at epoch 100 is  [[0.00271497]]\n",
      "loss at epoch 200 is  [[0.0034521]]\n",
      "loss at epoch 300 is  [[0.00381585]]\n",
      "loss at epoch 400 is  [[0.00402041]]\n",
      "loss at epoch 500 is  [[0.00416065]]\n",
      "loss at epoch 600 is  [[0.00426883]]\n",
      "loss at epoch 700 is  [[0.00436811]]\n",
      "loss at epoch 800 is  [[0.00451449]]\n",
      "loss at epoch 900 is  [[0.00465766]]\n",
      "loss at epoch 1000 is  [[0.00475037]]\n",
      "loss at epoch 1100 is  [[0.00479284]]\n",
      "loss at epoch 1200 is  [[0.00476691]]\n",
      "loss at epoch 1300 is  [[0.00423379]]\n",
      "loss at epoch 1400 is  [[0.00320144]]\n",
      "loss at epoch 1500 is  [[0.00266269]]\n",
      "loss at epoch 1600 is  [[0.0024408]]\n",
      "loss at epoch 1700 is  [[0.00233829]]\n",
      "loss at epoch 1800 is  [[0.0022839]]\n",
      "loss at epoch 1900 is  [[0.00225151]]\n",
      "loss at epoch 2000 is  [[0.00222866]]\n",
      "loss at epoch 2100 is  [[0.00220799]]\n",
      "loss at epoch 2200 is  [[0.00218466]]\n",
      "loss at epoch 2300 is  [[0.00215552]]\n",
      "loss at epoch 2400 is  [[0.00211911]]\n",
      "loss at epoch 2500 is  [[0.00207612]]\n",
      "loss at epoch 2600 is  [[0.00202944]]\n",
      "loss at epoch 2700 is  [[0.00198319]]\n",
      "loss at epoch 2800 is  [[0.00194099]]\n",
      "loss at epoch 2900 is  [[0.0019048]]\n",
      "loss at epoch 3000 is  [[0.00187492]]\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM(train_data=inputs, targets=targets, batch_size=4, debug=0, test=0)\n",
    "lstm.train(epoch=3001, lr=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bf3ddd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T00:48:46.761996Z",
     "start_time": "2021-11-26T00:48:46.726602Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Price : 118.88                             Next Price : 118.791 \n",
      "\n",
      "Current Price : 118.77                             Next Price : 118.691 \n",
      "\n",
      "Current Price : 118.69                             Next Price : 118.628 \n",
      "\n",
      "Current Price : 118.64                             Next Price : 118.651 \n",
      "\n",
      "Current Price : 118.77                             Next Price : 118.699 \n",
      "\n",
      "Current Price : 118.69                             Next Price : 118.619 \n",
      "\n",
      "Current Price : 118.64                             Next Price : 118.65 \n",
      "\n",
      "Current Price : 118.65                             Next Price : 118.575 \n",
      "\n",
      "Current Price : 118.69                             Next Price : 118.64 \n",
      "\n",
      "Current Price : 118.64                             Next Price : 118.643 \n",
      "\n",
      "Current Price : 118.65                             Next Price : 118.576 \n",
      "\n",
      "Current Price : 118.55                             Next Price : 118.553 \n",
      "\n",
      "Current Price : 118.64                             Next Price : 118.658 \n",
      "\n",
      "Current Price : 118.65                             Next Price : 118.574 \n",
      "\n",
      "Current Price : 118.55                             Next Price : 118.553 \n",
      "\n",
      "Current Price : 118.55                             Next Price : 118.572 \n",
      "\n",
      "Current Price : 118.65                             Next Price : 118.584 \n",
      "\n",
      "Current Price : 118.55                             Next Price : 118.552 \n",
      "\n",
      "Current Price : 118.55                             Next Price : 118.572 \n",
      "\n",
      "Current Price : 118.59                             Next Price : 118.728 \n",
      "\n",
      "Current Price : 118.55                             Next Price : 118.553 \n",
      "\n",
      "Current Price : 118.55                             Next Price : 118.572 \n",
      "\n",
      "Current Price : 118.59                             Next Price : 118.728 \n",
      "\n",
      "Current Price : 118.71                             Next Price : 118.851 \n",
      "\n",
      "Current Price : 118.55                             Next Price : 118.572 \n",
      "\n",
      "Current Price : 118.59                             Next Price : 118.728 \n",
      "\n",
      "Current Price : 118.71                             Next Price : 118.851 \n",
      "\n",
      "Current Price : 118.87                             Next Price : 118.782 \n",
      "\n",
      "Current Price : 118.59                             Next Price : 118.721 \n",
      "\n",
      "Current Price : 118.71                             Next Price : 118.849 \n",
      "\n",
      "Current Price : 118.87                             Next Price : 118.782 \n",
      "\n",
      "Current Price : 118.78                             Next Price : 118.817 \n",
      "\n",
      "Current Price : 118.71                             Next Price : 118.797 \n",
      "\n",
      "Current Price : 118.87                             Next Price : 118.788 \n",
      "\n",
      "Current Price : 118.78                             Next Price : 118.814 \n",
      "\n",
      "Current Price : 118.8                             Next Price : 118.877 \n",
      "\n",
      "Current Price : 118.87                             Next Price : 118.765 \n",
      "\n",
      "Current Price : 118.78                             Next Price : 118.809 \n",
      "\n",
      "Current Price : 118.8                             Next Price : 118.875 \n",
      "\n",
      "Current Price : 118.88                             Next Price : 118.82 \n",
      "\n",
      "Current Price : 118.78                             Next Price : 118.781 \n",
      "\n",
      "Current Price : 118.8                             Next Price : 118.866 \n",
      "\n",
      "Current Price : 118.88                             Next Price : 118.821 \n",
      "\n",
      "Current Price : 118.82                             Next Price : 118.771 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lstm.goPredict(inputs, opscaler, ipscaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8302bd10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
