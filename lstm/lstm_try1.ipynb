{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "41b9625b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T07:16:34.660887Z",
     "start_time": "2021-11-21T07:16:34.440065Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "3ba77827",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T08:17:59.865286Z",
     "start_time": "2021-11-21T08:17:59.851691Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_orthogonal(param):\n",
    "    \"\"\"\n",
    "    Initializes weight parameters orthogonally.\n",
    "    This is a common initiailization for recurrent neural networks.\n",
    "    \n",
    "    Refer to this paper for an explanation of this initialization:\n",
    "    https://arxiv.org/abs/1312.6120\n",
    "    \"\"\"\n",
    "    if param.ndim < 2:\n",
    "        raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
    "\n",
    "    rows, cols = param.shape\n",
    "    \n",
    "    new_param = np.random.randn(rows, cols)\n",
    "    \n",
    "    if rows < cols:\n",
    "        new_param = new_param.T\n",
    "    \n",
    "    # Compute QR factorization\n",
    "    q, r = np.linalg.qr(new_param)\n",
    "    \n",
    "    # Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\n",
    "    d = np.diag(r, 0)\n",
    "    ph = np.sign(d)\n",
    "    q *= ph\n",
    "\n",
    "    if rows < cols:\n",
    "        q = q.T\n",
    "    \n",
    "    new_param = q\n",
    "    \n",
    "    return new_param\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the element-wise sigmoid activation function for an array x.\n",
    "\n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    f = 1 / (1 + np.exp(-x_safe))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        return f * (1 - f)\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f\n",
    "    \n",
    "def tanh(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the element-wise tanh activation function for an array x.\n",
    "\n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    f = (np.exp(x_safe)-np.exp(-x_safe))/(np.exp(x_safe)+np.exp(-x_safe))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        return 1-f**2\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f\n",
    "    \n",
    "    \n",
    "def softmax(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the softmax for an array x.\n",
    "    \n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    f = np.exp(x_safe) / np.sum(np.exp(x_safe))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        pass # We will not need this one\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f\n",
    "    \n",
    "def clip_gradient_norm(grads, max_norm=0.25):\n",
    "    \"\"\"\n",
    "    Clips gradients to have a maximum norm of `max_norm`.\n",
    "    This is to prevent the exploding gradients problem.\n",
    "    \"\"\" \n",
    "    # Set the maximum of the norm to be of type float\n",
    "    max_norm = float(max_norm)\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculate the L2 norm squared for each gradient and add them to the total norm\n",
    "    for grad in grads:\n",
    "        grad_norm = np.sum(np.power(grad, 2))\n",
    "        total_norm += grad_norm\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Calculate clipping coeficient\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    \n",
    "    # If the total norm is larger than the maximum allowable norm, then clip the gradient\n",
    "    if clip_coef < 1:\n",
    "        for grad in grads:\n",
    "            grad *= clip_coef\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def update_parameters(params, grads, lr=1e-3):\n",
    "    # Take a step\n",
    "    for param, grad in zip(params, grads):\n",
    "        param -= lr * grad\n",
    "    \n",
    "    return params\n",
    "\n",
    "def mean_squared_error(actual, predicted):\n",
    "    sum_square_error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        sum_square_error += (actual[i] - predicted[i])**2.0\n",
    "    mean_square_error = 1.0 / len(actual) * sum_square_error\n",
    "    return mean_square_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a3bab1c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T07:50:22.680532Z",
     "start_time": "2021-11-21T07:50:22.661242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10]\n",
      " [11]\n",
      " [12]\n",
      " [11]\n",
      " [12]\n",
      " [10]\n",
      " [11]\n",
      " [10]]\n",
      "[11]\n"
     ]
    }
   ],
   "source": [
    "dataset=StringIO(\"\"\"Date,Open,High,Low,Close,Volume,Trade_count,Vwap\n",
    "2015-12-01 09:00:00+00:00,118.88,118.94,118.88,118.94,1145,5,118.902052\n",
    "2015-12-01 09:15:00+00:00,118.77,118.77,118.77,118.77,200,1,118.77\n",
    "2015-12-01 09:30:00+00:00,118.69,118.69,118.6,118.6,900,4,118.61\n",
    "2015-12-01 09:45:00+00:00,118.64,118.65,118.64,118.65,3580,5,118.648883\n",
    "2015-12-01 10:00:00+00:00,118.65,118.65,118.55,118.55,1820,4,118.611538\n",
    "2015-12-01 10:15:00+00:00,118.55,118.6,118.55,118.6,880,5,118.5625\n",
    "2015-12-01 10:30:00+00:00,118.55,118.55,118.5,118.5,1878,5,118.513312\n",
    "2015-12-01 10:45:00+00:00,118.59,118.72,118.59,118.72,2499,10,118.628431\n",
    "2015-12-01 11:00:00+00:00,118.71,118.9,118.71,118.9,2842,11,118.86064\n",
    "2015-12-01 11:15:00+00:00,118.87,118.87,118.87,118.87,300,2,118.87\n",
    "2015-12-01 11:30:00+00:00,118.78,118.8,118.76,118.8,3914,22,118.785876\n",
    "2015-12-01 11:45:00+00:00,118.8,118.99,118.77,118.9,7900,37,118.893542\n",
    "2015-12-01 12:00:00+00:00,118.88,118.98,118.84,118.84,6540,34,118.922648\n",
    "2015-12-01 12:15:00+00:00,118.82,118.84,118.77,118.77,5603,28,118.804962\n",
    "2015-12-01 12:30:00+00:00,118.77,118.89,118.76,118.88,7612,31,118.824002\n",
    "\"\"\")\n",
    "df = pd.read_table(dataset, sep=\",\")\n",
    "\n",
    "#ip = np.array([ [1,2,3],[6,8,9],[3,4,5],[4,7,8],[4,2,5],[5,7,4] ])\n",
    "#op = np.array([[2,8,4,7,2,4]])\n",
    "#op = op.reshape(6,1)\n",
    "ip = np.array([ [1],[2],[0],[2],[0],[1],[2],[1] ])\n",
    "op = np.array([ [10],[11],[12],[11],[12],[10],[11],[10] ])\n",
    "num_steps = 3\n",
    "num_features = 3\n",
    "#ip_shaped = np.reshape(ip, newshape=(-1, num_steps, num_features))\n",
    "\n",
    "#X = np.array([ [1,2,3, 4, 5, 6] ])\n",
    "#Y = np.array([[2,3,4,5,6,7]])\n",
    "print(op)\n",
    "print(op[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "80e2d907",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T07:50:23.553908Z",
     "start_time": "2021-11-21T07:50:23.543988Z"
    }
   },
   "outputs": [],
   "source": [
    "def lstm_data_transform(x_data, y_data, num_steps=2):\n",
    "    \"\"\" Changes data to the format for LSTM training \n",
    "for sliding window approach \"\"\"\n",
    "    # Prepare the list for the transformed data\n",
    "    X, y = list(), list()\n",
    "    # Loop of the entire data set\n",
    "    #print(x_data.shape[0])\n",
    "    for i in range(x_data.shape[0]):\n",
    "        # compute a new (sliding window) index\n",
    "        end_ix = i + num_steps\n",
    "\n",
    "        # if index is larger than the size of the dataset, we stop\n",
    "        #print(end_ix)\n",
    "        if end_ix >= x_data.shape[0]:\n",
    "            break\n",
    "        # Get a sequence of data for x\n",
    "        seq_X = x_data[i:end_ix]\n",
    "        #print(x_data[i:end_ix])\n",
    "        # Get only the last element of the sequency for y\n",
    "        #print(y_data[end_ix])\n",
    "        seq_y = y_data[end_ix-1]\n",
    "        # Append the list with sequencies\n",
    "        X.append(seq_X)\n",
    "        y.append(seq_y)\n",
    "        #print(X,y)\n",
    "    # Make final arrays\n",
    "    x_array = np.array(X)\n",
    "    y_array = np.array(y)\n",
    "    return x_array, y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9428a45e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T07:50:24.783406Z",
     "start_time": "2021-11-21T07:50:24.775366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1]\n",
      "  [2]]\n",
      "\n",
      " [[2]\n",
      "  [0]]\n",
      "\n",
      " [[0]\n",
      "  [2]]\n",
      "\n",
      " [[2]\n",
      "  [0]]\n",
      "\n",
      " [[0]\n",
      "  [1]]\n",
      "\n",
      " [[1]\n",
      "  [2]]]\n",
      "====\n",
      "[[11]\n",
      " [12]\n",
      " [11]\n",
      " [12]\n",
      " [10]\n",
      " [11]]\n"
     ]
    }
   ],
   "source": [
    "ipt,opt=lstm_data_transform(ip,op)\n",
    "print(ipt)\n",
    "print(\"====\")\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d0436a5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T08:21:58.508680Z",
     "start_time": "2021-11-21T08:21:58.489461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_f: (50, 51)\n",
      "W_i: (50, 51)\n",
      "W_g: (50, 51)\n",
      "W_o: (50, 51)\n",
      "W_v: (1, 50)\n",
      "b_i: (50, 1)\n",
      "b_g: (50, 1)\n",
      "b_o: (50, 1)\n",
      "b_v: (50, 1)\n"
     ]
    }
   ],
   "source": [
    "# Size of concatenated hidden + input vector\n",
    "hd = 50\n",
    "id = 1\n",
    "z = hd + id\n",
    "\n",
    "def init_lstm(hd, id, z):\n",
    "    \"\"\"\n",
    "    Initializes our LSTM network.\n",
    "    \n",
    "    Args:\n",
    "     `hidden_size`: the dimensions of the hidden state\n",
    "     `vocab_size`: the dimensions of our vocabulary\n",
    "     `z_size`: the dimensions of the concatenated input \n",
    "    \"\"\"\n",
    "    # Weight matrix (forget gate)\n",
    "    W_f = np.zeros((hd, z))\n",
    "   \n",
    "    # Bias for forget gate\n",
    "    b_f = np.zeros((hd, 1))\n",
    "\n",
    "    # Weight matrix (input gate)\n",
    "    W_i = np.zeros((hd, z))\n",
    "    \n",
    "    # Bias for input gate\n",
    "    b_i = np.zeros((hd, 1))\n",
    "\n",
    "    # Weight matrix (candidate)\n",
    "    W_g = np.zeros((hd, z))\n",
    "\n",
    "    # Bias for candidate\n",
    "    b_g = np.zeros((hd, 1))\n",
    "\n",
    "    # Weight matrix of the output gate\n",
    "    W_o = np.zeros((hd, z))\n",
    "    \n",
    "    # Bias for output gate\n",
    "    b_o = np.zeros((hd, 1))\n",
    "\n",
    "    # Weight matrix relating the hidden-state to the output\n",
    "    W_v = np.zeros((id, hd))\n",
    "    \n",
    "    # Bias for logits\n",
    "    b_v = np.zeros((id, 1))\n",
    "    \n",
    "    # Initialize weights according to https://arxiv.org/abs/1312.6120\n",
    "    W_f = init_orthogonal(W_f)\n",
    "    W_i = init_orthogonal(W_i)\n",
    "    W_g = init_orthogonal(W_g)\n",
    "    W_o = init_orthogonal(W_o)\n",
    "    W_v = init_orthogonal(W_v)\n",
    "\n",
    "    return W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v\n",
    "\n",
    "\n",
    "params = init_lstm(hd, id, z)\n",
    "print('W_f:', params[0].shape)\n",
    "print('W_i:', params[1].shape)\n",
    "print('W_g:', params[2].shape)\n",
    "print('W_o:', params[3].shape)\n",
    "print('W_v:', params[4].shape)\n",
    "print('b_i:', params[5].shape)\n",
    "print('b_g:', params[6].shape)\n",
    "print('b_o:', params[7].shape)\n",
    "print('b_v:', params[8].shape)\n",
    "\n",
    "for param in params:\n",
    "    assert param.ndim == 2, \\\n",
    "        'all parameters should be 2-dimensional '\\\n",
    "        '(hint: a dimension can simply have size 1)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "390421a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T08:22:03.003158Z",
     "start_time": "2021-11-21T08:22:02.989938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence:\n",
      "[[1]\n",
      " [2]]\n",
      "\n",
      "Target sequence:\n",
      "[11]\n",
      "\n",
      "Predicted sequence:\n",
      "[[0.00872794]]\n"
     ]
    }
   ],
   "source": [
    "hidden_size=hd\n",
    "def forward(inputs, h_prev, C_prev, p):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
    "    h_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    C_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
    "    p -- python list containing:\n",
    "                        W_f -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_f -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                        W_i -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_i -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "                        W_g -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_g --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                        W_o -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_o --  Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "                        W_v -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_v, n_a)\n",
    "                        b_v -- Bias relating the hidden-state to the output, numpy array of shape (n_v, 1)\n",
    "    Returns:\n",
    "    z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s -- lists of size m containing the computations in each forward pass\n",
    "    outputs -- prediction at timestep \"t\", numpy array of shape (n_v, m)\n",
    "    \"\"\"\n",
    "    assert h_prev.shape == (hidden_size, 1)\n",
    "    assert C_prev.shape == (hidden_size, 1)\n",
    "\n",
    "    # First we unpack our parameters\n",
    "    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p\n",
    "    \n",
    "    # Save a list of computations for each of the components in the LSTM\n",
    "    x_s, z_s, f_s, i_s,  = [], [] ,[], []\n",
    "    g_s, C_s, o_s, h_s = [], [] ,[], []\n",
    "    v_s, output_s =  [], [] \n",
    "    \n",
    "    # Append the initial cell and hidden state to their respective lists\n",
    "    h_s.append(h_prev)\n",
    "    C_s.append(C_prev)\n",
    "    \n",
    "    for x in inputs:\n",
    "        \n",
    "        # Concatenate input and hidden state\n",
    "        z = np.row_stack((h_prev, x))\n",
    "        z_s.append(z)\n",
    "        \n",
    "        # Calculate forget gate\n",
    "        f = sigmoid(np.dot(W_f, z) + b_f)\n",
    "        f_s.append(f)\n",
    "        \n",
    "        # Calculate input gate\n",
    "        i = sigmoid(np.dot(W_i, z) + b_i)\n",
    "        i_s.append(i)\n",
    "        \n",
    "        # Calculate candidate\n",
    "        g = tanh(np.dot(W_g, z) + b_g)\n",
    "        g_s.append(g)\n",
    "        \n",
    "        # Calculate memory state\n",
    "        C_prev = C_prev * f + g * i\n",
    "        C_s.append(C_prev)\n",
    "        \n",
    "        # Calculate output gate\n",
    "        o = sigmoid(np.dot(W_o, z) + b_o)\n",
    "        o_s.append(o)\n",
    "        \n",
    "        # Calculate hidden state\n",
    "        h_prev = o * tanh(C_prev)\n",
    "        h_s.append(h_prev)\n",
    "\n",
    "        # Calculate logits\n",
    "        v = np.dot(W_v, h_prev) + b_v\n",
    "\n",
    "\n",
    "    return z_s, f_s, i_s, g_s, C_s, o_s, h_s, v\n",
    "\n",
    "\n",
    "# Get first sentence in test set\n",
    "inputs, targets = ipt[0],opt[0]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "#inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "#targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "h = np.zeros((hidden_size, 1))\n",
    "c = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Forward pass\n",
    "z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s = forward(inputs, h, c, params)\n",
    "\n",
    "#output_sentence = [idx_to_word[np.argmax(output)] for output in outputs]\n",
    "print('Input sentence:')\n",
    "print(inputs)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(targets)\n",
    "\n",
    "print('\\nPredicted sequence:')\n",
    "print(v_s)\n",
    "outputs = v_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "53dc1f43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T08:07:25.584198Z",
     "start_time": "2021-11-21T08:07:25.566673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[ 0.01142451],\n",
       "        [-0.00536775],\n",
       "        [ 0.0018322 ],\n",
       "        [ 0.05113451],\n",
       "        [ 0.00326823],\n",
       "        [ 0.0248653 ],\n",
       "        [ 0.05708893],\n",
       "        [ 0.01105601],\n",
       "        [ 0.00979913],\n",
       "        [-0.03440035],\n",
       "        [ 0.03356718],\n",
       "        [ 0.01798711],\n",
       "        [ 0.04213724],\n",
       "        [ 0.01291847],\n",
       "        [-0.03075419],\n",
       "        [-0.01360676],\n",
       "        [-0.02867816],\n",
       "        [-0.02075099],\n",
       "        [-0.02124709],\n",
       "        [-0.03610321],\n",
       "        [-0.04425811],\n",
       "        [ 0.05159688],\n",
       "        [ 0.02503887],\n",
       "        [-0.03471498],\n",
       "        [-0.01700485],\n",
       "        [ 0.00293894],\n",
       "        [-0.03632866],\n",
       "        [ 0.04928718],\n",
       "        [-0.03982656],\n",
       "        [-0.00904508],\n",
       "        [ 0.00564272],\n",
       "        [ 0.05459933],\n",
       "        [-0.02513214],\n",
       "        [-0.00109002],\n",
       "        [ 0.04910779],\n",
       "        [ 0.00493877],\n",
       "        [ 0.03477176],\n",
       "        [-0.09503927],\n",
       "        [-0.0047194 ],\n",
       "        [-0.03233285],\n",
       "        [-0.00454196],\n",
       "        [-0.02089882],\n",
       "        [ 0.02615614],\n",
       "        [-0.02797773],\n",
       "        [ 0.04682275],\n",
       "        [ 0.0070009 ],\n",
       "        [-0.01720639],\n",
       "        [ 0.02598945],\n",
       "        [-0.03507819],\n",
       "        [-0.08364407]]),\n",
       " array([[ 0.02896663],\n",
       "        [-0.0042557 ],\n",
       "        [ 0.01775148],\n",
       "        [ 0.11151277],\n",
       "        [ 0.00405215],\n",
       "        [ 0.05803804],\n",
       "        [ 0.13133451],\n",
       "        [ 0.02084874],\n",
       "        [ 0.0145995 ],\n",
       "        [-0.07402522],\n",
       "        [ 0.06355983],\n",
       "        [ 0.04742244],\n",
       "        [ 0.07780857],\n",
       "        [ 0.02279281],\n",
       "        [-0.07669721],\n",
       "        [-0.04496165],\n",
       "        [-0.1002108 ],\n",
       "        [-0.05544074],\n",
       "        [-0.0499911 ],\n",
       "        [-0.09060777],\n",
       "        [-0.0938551 ],\n",
       "        [ 0.14012442],\n",
       "        [ 0.05294314],\n",
       "        [-0.08962829],\n",
       "        [-0.046812  ],\n",
       "        [-0.00060975],\n",
       "        [-0.08813158],\n",
       "        [ 0.11849756],\n",
       "        [-0.1127335 ],\n",
       "        [-0.05255913],\n",
       "        [ 0.02821019],\n",
       "        [ 0.12211388],\n",
       "        [-0.06041909],\n",
       "        [-0.01649208],\n",
       "        [ 0.13424918],\n",
       "        [ 0.00644534],\n",
       "        [ 0.10381926],\n",
       "        [-0.21176091],\n",
       "        [-0.02063588],\n",
       "        [-0.08365973],\n",
       "        [ 0.00182334],\n",
       "        [-0.05273779],\n",
       "        [ 0.06718134],\n",
       "        [-0.05899837],\n",
       "        [ 0.10726386],\n",
       "        [ 0.0171806 ],\n",
       "        [-0.02975838],\n",
       "        [ 0.06864729],\n",
       "        [-0.07065716],\n",
       "        [-0.19521446]])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[ 0.02222424],\n",
       "        [-0.01068647],\n",
       "        [ 0.00331574],\n",
       "        [ 0.10007769],\n",
       "        [ 0.0069369 ],\n",
       "        [ 0.04942714],\n",
       "        [ 0.106544  ],\n",
       "        [ 0.02157843],\n",
       "        [ 0.02084724],\n",
       "        [-0.07039985],\n",
       "        [ 0.07584543],\n",
       "        [ 0.03905357],\n",
       "        [ 0.09048596],\n",
       "        [ 0.02609532],\n",
       "        [-0.06043572],\n",
       "        [-0.02440227],\n",
       "        [-0.04834669],\n",
       "        [-0.03900166],\n",
       "        [-0.03982334],\n",
       "        [-0.07394315],\n",
       "        [-0.10337244],\n",
       "        [ 0.09370468],\n",
       "        [ 0.04993309],\n",
       "        [-0.07296355],\n",
       "        [-0.03799728],\n",
       "        [ 0.0058049 ],\n",
       "        [-0.07477437],\n",
       "        [ 0.09834417],\n",
       "        [-0.07983935],\n",
       "        [-0.01619693],\n",
       "        [ 0.01055029],\n",
       "        [ 0.12857579],\n",
       "        [-0.05101715],\n",
       "        [-0.00223276],\n",
       "        [ 0.09009943],\n",
       "        [ 0.00977328],\n",
       "        [ 0.07060174],\n",
       "        [-0.1749774 ],\n",
       "        [-0.00934092],\n",
       "        [-0.06236427],\n",
       "        [-0.00944215],\n",
       "        [-0.04328561],\n",
       "        [ 0.05765865],\n",
       "        [-0.05928902],\n",
       "        [ 0.10340916],\n",
       "        [ 0.01416188],\n",
       "        [-0.03450638],\n",
       "        [ 0.05353319],\n",
       "        [-0.07716411],\n",
       "        [-0.17260287]]),\n",
       " array([[ 0.0556138 ],\n",
       "        [-0.00849062],\n",
       "        [ 0.02966073],\n",
       "        [ 0.21789516],\n",
       "        [ 0.00892247],\n",
       "        [ 0.11490751],\n",
       "        [ 0.23266611],\n",
       "        [ 0.03990632],\n",
       "        [ 0.03325935],\n",
       "        [-0.15781978],\n",
       "        [ 0.16885534],\n",
       "        [ 0.11165189],\n",
       "        [ 0.1817919 ],\n",
       "        [ 0.04764222],\n",
       "        [-0.14799554],\n",
       "        [-0.07217378],\n",
       "        [-0.14707288],\n",
       "        [-0.10010411],\n",
       "        [-0.0896112 ],\n",
       "        [-0.19732516],\n",
       "        [-0.27026599],\n",
       "        [ 0.23576669],\n",
       "        [ 0.10652838],\n",
       "        [-0.19954304],\n",
       "        [-0.11716904],\n",
       "        [-0.00115267],\n",
       "        [-0.1828059 ],\n",
       "        [ 0.23031752],\n",
       "        [-0.23382127],\n",
       "        [-0.0858874 ],\n",
       "        [ 0.05040576],\n",
       "        [ 0.37241954],\n",
       "        [-0.1270132 ],\n",
       "        [-0.03434558],\n",
       "        [ 0.23146818],\n",
       "        [ 0.01264385],\n",
       "        [ 0.2149203 ],\n",
       "        [-0.38449395],\n",
       "        [-0.03957304],\n",
       "        [-0.15687155],\n",
       "        [ 0.00393724],\n",
       "        [-0.11396058],\n",
       "        [ 0.16169698],\n",
       "        [-0.13685926],\n",
       "        [ 0.25897756],\n",
       "        [ 0.03530983],\n",
       "        [-0.06002449],\n",
       "        [ 0.14405327],\n",
       "        [-0.17508969],\n",
       "        [-0.42694932]])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04689127]\n"
     ]
    }
   ],
   "source": [
    "display(h_s)\n",
    "display(C_s)\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "927ec13c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T08:22:04.296833Z",
     "start_time": "2021-11-21T08:22:04.272005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We get a loss of:\n",
      "120.8080614520397\n"
     ]
    }
   ],
   "source": [
    "def backward(z, f, i, g, C, o, h, v, outputs, targets, p = params):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    z -- your concatenated input data  as a list of size m.\n",
    "    f -- your forget gate computations as a list of size m.\n",
    "    i -- your input gate computations as a list of size m.\n",
    "    g -- your candidate computations as a list of size m.\n",
    "    C -- your Cell states as a list of size m+1.\n",
    "    o -- your output gate computations as a list of size m.\n",
    "    h -- your Hidden state computations as a list of size m+1.\n",
    "    v -- your logit computations as a list of size m.\n",
    "    outputs -- your outputs as a list of size m.\n",
    "    targets -- your targets as a list of size m.\n",
    "    p -- python list containing:\n",
    "                        W_f -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_f -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
    "                        W_i -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_i -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
    "                        W_g -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_g --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
    "                        W_o -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
    "                        b_o --  Bias of the output gate, numpy array of shape (n_a, 1)\n",
    "                        W_v -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_v, n_a)\n",
    "                        b_v -- Bias relating the hidden-state to the output, numpy array of shape (n_v, 1)\n",
    "    Returns:\n",
    "    loss -- crossentropy loss for all elements in output\n",
    "    grads -- lists of gradients of every element in p\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack parameters\n",
    "    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p\n",
    "\n",
    "    # Initialize gradients as zero\n",
    "    W_f_d = np.zeros_like(W_f)\n",
    "    b_f_d = np.zeros_like(b_f)\n",
    "\n",
    "    W_i_d = np.zeros_like(W_i)\n",
    "    b_i_d = np.zeros_like(b_i)\n",
    "\n",
    "    W_g_d = np.zeros_like(W_g)\n",
    "    b_g_d = np.zeros_like(b_g)\n",
    "\n",
    "    W_o_d = np.zeros_like(W_o)\n",
    "    b_o_d = np.zeros_like(b_o)\n",
    "\n",
    "    W_v_d = np.zeros_like(W_v)\n",
    "    b_v_d = np.zeros_like(b_v)\n",
    "    \n",
    "    # Set the next cell and hidden state equal to zero\n",
    "    dh_next = np.zeros_like(h[0])\n",
    "    dC_next = np.zeros_like(C[0])\n",
    "        \n",
    "    # Track loss\n",
    "    loss = 0\n",
    "    \n",
    "    for t in reversed(range(len(outputs))):\n",
    "        \n",
    "        # Compute the cross entropy\n",
    "        #print(outputs[t][0], targets[t], -np.mean(np.log(outputs[t][0]) * targets[t]))\n",
    "        loss += (np.square(outputs[t] - targets[t])).mean(axis=None)\n",
    "        #loss += -np.mean(np.log(outputs[t]) * targets[t])\n",
    "        # Get the previous hidden cell state\n",
    "        C_prev= C[t-1]\n",
    "        \n",
    "        # Compute the derivative of the relation of the hidden-state to the output gate\n",
    "        dv = np.copy(outputs[t])\n",
    "        dv[np.argmax(targets[t])] -= 1\n",
    "\n",
    "        # Update the gradient of the relation of the hidden-state to the output gate\n",
    "        W_v_d += np.dot(dv, h[t].T)\n",
    "        b_v_d += dv\n",
    "\n",
    "        # Compute the derivative of the hidden state and output gate\n",
    "        dh = np.dot(W_v.T, dv)\n",
    "        dh = dh.reshape(hd,1) ##NEWLYADDED\n",
    "        dh += dh_next\n",
    "        do = dh * tanh(C[t])\n",
    "        do = sigmoid(o[t], derivative=True)*do\n",
    "        \n",
    "        # Update the gradients with respect to the output gate\n",
    "        W_o_d += np.dot(do, z[t].T)\n",
    "        b_o_d += do\n",
    "\n",
    "        # Compute the derivative of the cell state and candidate g\n",
    "        dC = np.copy(dC_next)\n",
    "        dC += dh * o[t] * tanh(tanh(C[t]), derivative=True)\n",
    "        dg = dC * i[t]\n",
    "        dg = tanh(g[t], derivative=True) * dg\n",
    "        \n",
    "        # Update the gradients with respect to the candidate\n",
    "        W_g_d += np.dot(dg, z[t].T)\n",
    "        b_g_d += dg\n",
    "\n",
    "        # Compute the derivative of the input gate and update its gradients\n",
    "        di = dC * g[t]\n",
    "        di = sigmoid(i[t], True) * di\n",
    "        W_i_d += np.dot(di, z[t].T)\n",
    "        b_i_d += di\n",
    "\n",
    "        # Compute the derivative of the forget gate and update its gradients\n",
    "        df = dC * C_prev\n",
    "        df = sigmoid(f[t]) * df\n",
    "        W_f_d += np.dot(df, z[t].T)\n",
    "        b_f_d += df\n",
    "\n",
    "        # Compute the derivative of the input and update the gradients of the previous hidden and cell state\n",
    "        dz = (np.dot(W_f.T, df)\n",
    "             + np.dot(W_i.T, di)\n",
    "             + np.dot(W_g.T, dg)\n",
    "             + np.dot(W_o.T, do))\n",
    "        dh_prev = dz[:hidden_size, :]\n",
    "        dC_prev = f[t] * dC\n",
    "        \n",
    "    grads= W_f_d, W_i_d, W_g_d, W_o_d, W_v_d, b_f_d, b_i_d, b_g_d, b_o_d, b_v_d\n",
    "    \n",
    "    # Clip gradients\n",
    "    grads = clip_gradient_norm(grads)\n",
    "    \n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "# Perform a backward pass\n",
    "loss, grads = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets, params)\n",
    "\n",
    "print('We get a loss of:')\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "7eb41793",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T08:22:21.614671Z",
     "start_time": "2021-11-21T08:22:21.169914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, validation loss: 125.42283594606177\n",
      "[[0.00516114]]\n",
      "Epoch 10, validation loss: 95.82671764544098\n",
      "[[1.417194]]\n",
      "Epoch 20, validation loss: 70.16013977797385\n",
      "[[2.83620672]]\n",
      "Epoch 30, validation loss: 48.499181421052896\n",
      "[[4.25680571]]\n",
      "Epoch 40, validation loss: 30.881054030361497\n",
      "[[5.67420627]]\n",
      "Epoch 50, validation loss: 17.297059774955283\n",
      "[[7.08678128]]\n",
      "Epoch 60, validation loss: 7.72009782878229\n",
      "[[8.49532337]]\n",
      "Epoch 70, validation loss: 2.126248937828942\n",
      "[[9.90141267]]\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "num_epochs = 80\n",
    "\n",
    "# Initialize a new network\n",
    "z_size = hd + id # Size of concatenated hidden + input vector\n",
    "params = init_lstm(hd, id, z_size)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Track loss\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "# For each epoch\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    # For each sentence in validation set\n",
    "    for inputs in ipt:\n",
    "        #print(inputs)\n",
    "        target = opt[count]\n",
    "        #print(targets)\n",
    "        # Initialize hidden state and cell state as zeros\n",
    "        h = np.zeros((hidden_size, 1))\n",
    "        c = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # Forward pass\n",
    "        z_s, f_s, i_s, g_s, C_s, o_s, h_s, outputs = forward(inputs, h, c, params)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss, _ = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, target, params)\n",
    "\n",
    "        # Update parameters\n",
    "        params = update_parameters(params, grads, lr=1e-1)\n",
    "\n",
    "        # Update loss\n",
    "        #print(loss)\n",
    "        epoch_validation_loss += loss\n",
    "        count+=1\n",
    "    \n",
    "#     # For each sentence in training set\n",
    "#     for inputs, targets in training_set:\n",
    "        \n",
    "#         # One-hot encode input and target sequence\n",
    "#         inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "#         targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "#         # Initialize hidden state and cell state as zeros\n",
    "#         h = np.zeros((hidden_size, 1))\n",
    "#         c = np.zeros((hidden_size, 1))\n",
    "\n",
    "#         # Forward pass\n",
    "#         z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params)\n",
    "        \n",
    "#         # Backward pass\n",
    "#         loss, grads = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params)\n",
    "        \n",
    "#         # Update parameters\n",
    "#         params = update_parameters(params, grads, lr=1e-1)\n",
    "        \n",
    "#         # Update loss\n",
    "#         epoch_training_loss += loss\n",
    "                \n",
    "#     # Save loss for plot\n",
    "#     training_loss.append(epoch_training_loss/len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss/len(ipt))\n",
    "    #print(epoch_validation_loss)\n",
    "    # Print loss every 10 epochs\n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch {i}, validation loss: {validation_loss[-1]}')\n",
    "        print(outputs)\n",
    "        #print(len(ipt))\n",
    "#        print(targets)\n",
    "\n",
    "    \n",
    "# # Get first sentence in test set\n",
    "# inputs, targets = ipt[1],opt[1]\n",
    "\n",
    "# # Initialize hidden state as zeros\n",
    "# h = np.zeros((hidden_size, 1))\n",
    "# c = np.zeros((hidden_size, 1))\n",
    "\n",
    "# # Forward pass\n",
    "# z_s, f_s, i_s, g_s, C_s, o_s, h_s, outputs = forward(inputs, h, c, params)\n",
    "\n",
    "# # Print example\n",
    "# print('Input sentence:')\n",
    "# print(inputs)\n",
    "\n",
    "# print('\\nTarget sequence:')\n",
    "# print(targets)\n",
    "\n",
    "# print('\\nPredicted sequence:')\n",
    "# print(outputs)\n",
    "\n",
    "# Plot training and validation loss\n",
    "# epoch = np.arange(len(training_loss))\n",
    "# plt.figure()\n",
    "# #plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "# plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "3d7860d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-21T08:23:23.014899Z",
     "start_time": "2021-11-21T08:23:23.005042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]]\n",
      "====\n",
      "[[11.18936986]]\n",
      "$$$$$\n"
     ]
    }
   ],
   "source": [
    "inp = ipt[5]\n",
    "print(inp)\n",
    "print(\"====\")\n",
    "z_s, f_s, i_s, g_s, C_s, o_s, h_s, outputs = forward(inp, h, c, params)\n",
    "print(outputs)\n",
    "print(\"$$$$$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819eca32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
